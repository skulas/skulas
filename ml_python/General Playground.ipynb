{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to work on the playgournd venv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Currently playground venv:\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_FOLDER = '/Users/skulas/Dev/TEMP/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "if not x:\n",
    "    print('when x is None if x returns False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_long_line = (f'Super heroes no pueden ser escritos '\n",
    "                  f'en {\"una sola\" if False else \"varias\"} linea')\n",
    "print(very_long_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class the_class:\n",
    "    def __init__(self, val):\n",
    "        self._prop_capsule = None\n",
    "        self.val = val\n",
    "    \n",
    "    @property\n",
    "    def prop_name(self):\n",
    "        if self._prop_capsule is None:\n",
    "            print('CREATED')\n",
    "            self._prop_capsule = {'the':'property', 'val':self.val}\n",
    "        return self._prop_capsule\n",
    "    \n",
    "the_inst = the_class('Amores Perros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(the_inst.prop_name)\n",
    "\n",
    "# Will Crash: the_inst.prop_name = 'lolo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [1,2,3,4,5,66,7,4,3]\n",
    "print(lista)\n",
    "lista.append(313)\n",
    "print(lista)\n",
    "lista.extend([432,213])\n",
    "print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [{'q':1, 'a':0},{'q':11, 'a':10},{'q':21, 'a':20},{'q':31, 'a':None},{'q':41, 'a':40},{'q':51, 'a':None}]\n",
    "\n",
    "nonone = [(o['q'], o['a']) for o in lista if o['a'] is not None]\n",
    "nones = [o['q'] for o in lista if o['a'] is None]\n",
    "display(nonone)\n",
    "display(nones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict from List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['k1','k2','k3','k4','k5']\n",
    "dic = dict((el,{}) for el in lista)\n",
    "dic['k1']['lolo']=12\n",
    "dic['k1']['loca']='super'\n",
    "print(dic)\n",
    "print(len(dic.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(*args):\n",
    "    print(args)\n",
    "\n",
    "print_args('1', 2, '4')\n",
    "arr = ['1', 2, '3']\n",
    "print_args(arr)\n",
    "print_args(*arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets and Sub lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return True if sub_lst is found inside lst.\n",
    "\"\"\"\n",
    "def is_sub_set(lst, sub_lst):\n",
    "    try:\n",
    "        ix = lst.index(sub_lst[0])\n",
    "    except ValueError:\n",
    "        return False\n",
    "    is_sbst = True\n",
    "    offset = ix\n",
    "    ix += 1\n",
    "    while is_sbst and (ix < len(lst)) and ((ix - offset) < len(sub_lst)):\n",
    "        if sub_lst[ix - offset] != lst[ix]:\n",
    "            try:\n",
    "                ix = lst[ix:].index(sub_lst[0]) + ix\n",
    "                offset = ix\n",
    "            except ValueError:\n",
    "                is_sbst = False\n",
    "\n",
    "        ix += 1\n",
    "        if is_sbst and (ix - offset + 1) < len(sub_lst) and ((ix + 1) == len(lst)):\n",
    "            is_sbst = False\n",
    "\n",
    "    return is_sbst\n",
    "\"\"\"\n",
    "Find series of numbers smaller than _thres_ in _lst_ of _number_of_items_ items length\n",
    "\"\"\"\n",
    "def has_series_of_small_numbers(lst, thres, number_of_items):\n",
    "    counter = 1  # count from 1 so comparison can be strong (without equal condition)\n",
    "    ix = 0\n",
    "    for item in lst:\n",
    "        if item < thres:\n",
    "            counter += 1\n",
    "            if counter > number_of_items:\n",
    "                # Why +2: +1 for counter = 1 and +1 coz ix+1 happens at the end and counter+1 at the beginning\n",
    "                return ix - counter + 2\n",
    "        else:\n",
    "            counter = 1\n",
    "        ix += 1\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "lista = [5,2,3,4,3,5,5,5,4,5,6,7]\n",
    "# sub_lista = [4,5,5,5]\n",
    "sub_lista = [4,5,7]\n",
    "res = all(el in lista for el in sub_lista)\n",
    "print(f\"All items in sub_lista {'are' if res else 'are not'} in lista\")\n",
    "\n",
    "res = set(sub_lista).issubset(set(lista))\n",
    "\n",
    "print(f\"sub_lista {'is' if res else 'is not'} a sub_set of lista\")\n",
    "      \n",
    "res = is_sub_set(lista, sub_lista)\n",
    "print(res)\n",
    "      \n",
    "thres = 5\n",
    "number_of_items = 4\n",
    "res = has_series_of_small_numbers(lista, thres, number_of_items)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections - Lists analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collections.Counter lets you find the most common\n",
    "# elements in an iterable:\n",
    "\n",
    "import collections\n",
    "c = collections.Counter('helloworld')\n",
    "\n",
    "display(c)\n",
    "\n",
    "display(c.most_common(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "\n",
    "print('range(6)')\n",
    "for i in range(6):\n",
    "    print(i, end=', ')\n",
    "\n",
    "print('\\n')\n",
    "print('range(1,4)')\n",
    "for i in range(1,4):\n",
    "    print(i, end=', ')\n",
    "\n",
    "print('\\n')\n",
    "print('range(2,9,3)')\n",
    "for i in range(2,9,3):\n",
    "    print(i, end=', ')\n",
    "\n",
    "print('\\n')\n",
    "print('Float Range:')\n",
    "for i in arange(5.5, 15.5, 2.5):\n",
    "    print(i, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(7%2)\n",
    "lst = [1,2,3,4,5,65,16,6,3,32,4,5]\n",
    "item_ix = 34532745\n",
    "print(lst[item_ix%len(lst)])\n",
    "item_ix = 1374532789\n",
    "print(lst[item_ix%len(lst)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import seed\n",
    "from random import sample\n",
    "\n",
    "for x in range(10):\n",
    "  print(random.randint(1,101))\n",
    "\n",
    "# select a random sample without replacement\n",
    "# seed random number generator\n",
    "seed(333)\n",
    "# prepare a sequence\n",
    "sequence = [i for i in range(20)]\n",
    "print(sequence)\n",
    "# select a subset without replacement\n",
    "rand_arr = sample(sequence, 5)\n",
    "display(rand_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statistics \n",
    "import pandas as pd\n",
    "  \n",
    "# Std\n",
    "sample = [1, 2, 3, 4, 5] \n",
    "# xbar is set to default value of 1 \n",
    "print(f\"Standard Deviation of sample is {statistics.stdev(sample)}\") \n",
    "    \n",
    "sample = [1, 2, 3, 4, 5, 21, 22, 23, 24, 25] \n",
    "print(f\"Standard Deviation of sample is {statistics.stdev(sample)}\")\n",
    "\n",
    "# Std of constant\n",
    "sample = [100, 100.2, 101.3, 101, 100.9, 100.85, 102] \n",
    "print(f\"Standard Deviation of sample is {statistics.stdev(sample)}\")\n",
    "\n",
    "sample = [0, 0.2, 1.3, 1, 0.9, 0.85, 2] \n",
    "print(f\"Standard Deviation of sample is {statistics.stdev(sample)}\")\n",
    "\n",
    "# s = pd.Series(sample)\n",
    "# s.rolling(3).std()\n",
    "\n",
    "sample = [100, 102.2, 110.3, 112, 120.9, 122.85, 130, 132, 140, 141.85, 150, 151.83, 160, 161.9, 170.15, 172] \n",
    "# sample = [100, 100.2, 101.3, 101, 100.9, 100.85, 102, 2000, 1057, 100, 100.2, 101.3, 101, 100.9, 100.85, 102] \n",
    "s = pd.Series(sample)\n",
    "stds = s.rolling(5).std()\n",
    "print(type(stds))\n",
    "print('----1----')\n",
    "print(stds[stds < 500])\n",
    "print('----2----')\n",
    "print(stds[stds < 500].index)\n",
    "print('----3----')\n",
    "print(s[stds < 500].max())\n",
    "print(s[stds < 500].min())\n",
    "print(s[stds < 500].median())\n",
    "print(s[stds < 500].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By ref, By Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class momoi:\n",
    "    counter = 1\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.number = momoi.counter\n",
    "        momoi.counter += 1\n",
    "    \n",
    "    def run_test(self):\n",
    "        v = self.x\n",
    "        self.x += 1\n",
    "        self.print_self(v)\n",
    "\n",
    "    def print_self(self, v):\n",
    "        print(f'I am {self.number} and this is v: {v} and self.x = {self.x} thus self.x is copied by value')\n",
    "        \n",
    "momo1 = momoi(12)\n",
    "momo2 = momoi(3)\n",
    "momo1.run_test()\n",
    "momo2.run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ptr = open('/users/ezequiellowi/dev/TEMP/SOURCES.txt', 'r')\n",
    "print(dir(file_ptr))\n",
    "fline1 = file_ptr.readline()\n",
    "fline2 = file_ptr.readline()\n",
    "file_ptr.close()\n",
    "file_ptr.name\n",
    "print(f'\\nFile {file_ptr.name} was closed. This is the first lines:')\n",
    "print(f'{fline1}{fline2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_path='/Users/ezequiellowi/workspace-new/tagging-server/data.json'\n",
    "with open(json_path) as json_file:\n",
    "    missing_counter = json.load(json_file)\n",
    "# display(missing_counter)\n",
    "missing_cntrs_df = pd.DataFrame.from_dict(missing_counter, orient='index')\n",
    "missing_cntrs_df = missing_cntrs_df.sort_values(by=0,ascending=False)\n",
    "display(missing_cntrs_df)\n",
    "missing_cntrs_df.to_csv('/Users/ezequiellowi/workspace-new/tagging-server/missing_counters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_list = ['spend', 'p2p']\n",
    "act = '-'.join(activities_list)\n",
    "print(act)\n",
    "empty_df = pd.DataFrame()\n",
    "print(empty_df.empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### DELETE THIS\n",
    "# ppp = pd.read_pickle('/Users/skulas/Dev/kre_cache_folder/payout_spenders_wvt_True_wht_False_2-2019_odp_True.pkl')\n",
    "# ppp = pd.read_pickle('/Users/skulas/Dev/kre_cache_folder/payout_spenders_wvt_False_wht_True_2-2019_odp_True.pkl')\n",
    "ppp = pd.read_pickle('/Users/skulas/Dev/kre_cache_folder/payout_act_spend-p2p_wvt_False_wht_True_2-2019_odp_True.pkl')\n",
    "\n",
    "print(ppp.shape)\n",
    "display(ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{'a':3},{'a':3},{'a':3},{'a':5},{'a':3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2,6,8,2]\n",
    "l2 = [3,2,5,8]\n",
    "\n",
    "#     v  `filter` returns the a iterator object. Here I'm type-casting \n",
    "#     v  it to `list` in order to display the resultant value\n",
    "print(list(filter(lambda x: x not in l2, l1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 9\n",
    "def dictic(num):\n",
    "    return dict(name=dict(uno=num+1, dos=num+2, tres=num+3,nombre='cca {}'.format(num)))\n",
    "\n",
    "lista = list()\n",
    "# lista = [dictic(0)]*20\n",
    "for i in range(0,12):\n",
    "    lista.append(dictic(num))\n",
    "    num+=1\n",
    "print(lista[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "# Displaying dataframes side by side\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                         'foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': [1]*12,\n",
    "                    'D': range(1,13)})\n",
    "display(df)\n",
    "list_c = df['C'].tolist()\n",
    "display(list_c)\n",
    "print('*** Columms of Numbers ***')\n",
    "num_cols = df.select_dtypes(np.number)\n",
    "display(num_cols)\n",
    "\n",
    "display(f'{\"*\"*8} Slicing {\"*\"*8}')\n",
    "display(df[2:2+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['foo', 'bar'],\n",
    "                    'B': ['one', 'one'],\n",
    "                    'C': [1]*2,\n",
    "                    'D': range(1,3)})\n",
    "df2 = pd.DataFrame({'A': ['shu', 'iop'],\n",
    "                    'B': ['two', 'two'],\n",
    "                    'C': [2]*2,\n",
    "                    'D': range(2,2*3,2)})\n",
    "df3 = pd.DataFrame({'A': ['dre', 'tre'],\n",
    "                    'B': ['three', 'three'],\n",
    "                    'C': [3]*2,\n",
    "                    'D': range(3,3*3,3)})\n",
    "\n",
    "display_side_by_side(df1, df2, df3)\n",
    "arr = [df1, df2, df3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "[Useful answer](https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas) on stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                         'foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': [1]*12,\n",
    "                    'D': range(1,13)})\n",
    "\n",
    "res = df.loc[df['B'] == 'two']\n",
    "display(res)\n",
    "res_bar = df.loc[(df['B'] == 'two') & (df['A'] == 'bar')]\n",
    "display(res_bar)\n",
    "display(df[['A','D']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'U':range(10,15), 'P':range(0,13,3), 'W':[2]*5})\n",
    "\n",
    "## Devide all rows by last row\n",
    "fractions_df = df.iloc[0:-1] / df.iloc[-1]\n",
    "\n",
    "## Subset\n",
    "sub_df = df.iloc[[1,4,3]]\n",
    "display_side_by_side(df, fractions_df, sub_df)\n",
    "\n",
    "## Address by index and column\n",
    "df2 = df.set_index('U')\n",
    "display(df2)\n",
    "display(f\"column name: df[12]['P']={df2.loc[12]['P']}\")\n",
    "display(f\"column number: df[12][0]={df2.loc[12][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum - Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'U':range(1,6), 'P':range(0,13,3), 'W':[2]*5})\n",
    "\n",
    "## Horizontal\n",
    "total_r = df.sum(numeric_only=True, axis=1)\n",
    "total_r.name = 'Total'\n",
    "df_h_total = df\n",
    "df_h_total[total_r.name] = total_r\n",
    "display(df_h_total)\n",
    "\n",
    "## Vertical\n",
    "total_c = df.sum(numeric_only=True, axis=0)\n",
    "total_c.name = 'Total'\n",
    "df_v_total = df_h_total.append(total_c)\n",
    "display(df_v_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_v_total.columns\n",
    "display(cols[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame([{'source':'1', 'destination':'2', 'total':13, 'tx_num':2},\n",
    "                  {'source':'13', 'destination':'3', 'total':313, 'tx_num':32}])\n",
    "\n",
    "df1 = pd.DataFrame([{'source':'1', 'way':'2', 'total':13, 'tx_num':2},\n",
    "                  {'tx_num':32, 'source':'13', 'way':'3'}])\n",
    "df2 = pd.DataFrame([{'destination':'1c1'},\n",
    "                  {'destination': '32u'}])\n",
    "\n",
    "new_df = pd.concat([df,df1,df2], axis=0, ignore_index=True, sort=False)\n",
    "display(new_df)\n",
    "\n",
    "# Breaks the index\n",
    "new_df1 = new_df.append(df1)\n",
    "display(new_df1)\n",
    "\n",
    "new_df2 = pd.concat([new_df, df1], sort=False)\n",
    "display(new_df2)\n",
    "\n",
    "cols, _ = new_df2.shape\n",
    "new_col = [np.nan]*cols\n",
    "new_col[1] = 'solo aca'\n",
    "new_df2['otro']=new_col\n",
    "display(new_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Cell Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stam = pd.DataFrame([{'source':'1', 'destination':'2', 'total':13, 'tx_num':2},\n",
    "                  {'source':'13', 'destination':'3', 'total':313, 'tx_num':32}])\n",
    "orig_sam = pd.DataFrame([{'source':'1', 'destination':'2', 'total':13, 'tx_num':2},\n",
    "                  {'source':'13', 'destination':'3', 'total':313, 'tx_num':32}])\n",
    "\n",
    "stam.at[0, 'destination']=12321\n",
    "\n",
    "display_side_by_side(orig_sam, stam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stam = pd.DataFrame([{'source':'1', 'destination':'2', 'total':13, 'tx_num':2},\n",
    "                  {'source':'4', 'destination':'3', 'total':132, 'tx_num':21},\n",
    "                  {'source':'4', 'destination':'2', 'total':132, 'tx_num':21},\n",
    "                  {'source':'13', 'destination':'2', 'total':313, 'tx_num':32},\n",
    "                  {'source':'13', 'destination':'3', 'total':313, 'tx_num':32}])\n",
    "stam.set_index(['source', 'destination'], inplace=True)\n",
    "\n",
    "stam2 = pd.DataFrame([{'source':'1', 'destination':'2', 'total':13, 'tx_num':2},\n",
    "                  {'source':'4', 'destination':'3', 'total':132, 'tx_num':21},\n",
    "                  {'source':'4', 'destination':'4', 'total':132, 'tx_num':21},\n",
    "                  {'source':'13', 'destination':'3', 'total':313, 'tx_num':32},\n",
    "                  {'source':'13', 'destination':'4', 'total':313, 'tx_num':32}])\n",
    "stam2.set_index(['source', 'destination'], inplace=True)\n",
    "\n",
    "\n",
    "display(stam, stam2)\n",
    "df_diff = stam2.index.difference(stam.index)\n",
    "display(df_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = { 'United States': 157, 'United Kingdom': 93, 'Japan': 89, 'China': 63,\n",
    "      'Germany': 44, 'India': 42, 'Italy': 40, 'Australia': 35, 'Brazil': 32,\n",
    "      'France': 31, 'Taiwan': 31, 'Spain': 29 }\n",
    "\n",
    "data = pd.Series(x).reset_index(name='value')\n",
    "data2 = data.rename(columns={'index':'country'})\n",
    "display_side_by_side(data, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = { 'United States': 157, 'United Kingdom': 93, 'Japan': 89, 'China': 63,\n",
    "#       'Germany': 44, 'India': 42, 'Italy': 40, 'Australia': 35, 'Brazil': 32,\n",
    "#       'France': 31, 'Taiwan': 31, 'Spain': 29 }\n",
    "xx = list(range(0,10))\n",
    "x = {'x':xx, 'y':[pow(x,2) for x in xx]}\n",
    "\n",
    "dfx = pd.DataFrame(x)\n",
    "dfy = pd.DataFrame(x)\n",
    "# dfx = pd.Series(x).reset_index(name='value').rename(columns={'index':'country'})\n",
    "# dfy = pd.Series(x).reset_index(name='value').rename(columns={'index':'country'})\n",
    "\n",
    "dfy['y'] = dfy['y'].shift(1, axis=0, fill_value=0)\n",
    "ratio = dfx['y']/dfy['y']\n",
    "sqratio = [pow(rat, 1/2) for rat in ratio]\n",
    "diff = dfx['y']-dfy['y']\n",
    "compdf = pd.DataFrame()\n",
    "compdf['ratio'] = ratio\n",
    "compdf['diff'] = diff\n",
    "compdf['sratio'] = sqratio\n",
    "\n",
    "display_side_by_side(dfx,dfy, compdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                         'foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': [1]*12,\n",
    "                    'D': range(1,13)})\n",
    " \n",
    "\n",
    "display(df)\n",
    "\n",
    "grouped = df.groupby(['A', 'B'])\n",
    "grp_sum = grouped.sum()\n",
    "display(grp_sum)\n",
    "\n",
    "grouped2 = df.groupby(['B', 'A'])\n",
    "grp_sum2 = grouped2.sum()\n",
    "grp_count2 = grouped2.count()\n",
    "print('SUM:')\n",
    "display(grp_sum2)\n",
    "print('-----------')\n",
    "print('COUNT:')\n",
    "display(grp_count2)\n",
    "print('TOTAL: {}'.format(grp_count2['C'].sum()))\n",
    "\n",
    "print('foo, one')\n",
    "display(grp_sum.loc[('foo','one')])\n",
    "\n",
    "gg = grp_sum2.loc[('one','foo')]\n",
    "print('one, foo')\n",
    "print('D = {}, C = {}'.format(gg['D'], gg['C']))\n",
    "\n",
    "single_group = df.groupby(['A'])\n",
    "sing_sum = single_group.sum()\n",
    "display(sing_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grp_count2) # grp_count2 From block above\n",
    "ren = grp_count2.rename(index={'one':'Tetas', 'two':'Culo', 'foo':'Cerveza'}, columns={'C':'Unos', 'D':'Otros'})\n",
    "# ren.columns = ren.columns.set_levels([['Primo', 'Secu']], ren.columns.levels[1])\n",
    "# ren.index.set_names(names=['Primo', 'Secu'], level=1)\n",
    "ren.index = ren.index.set_names('Primo', level=0)\n",
    "ren.index = ren.index.set_names('Secu', level=1)\n",
    "display(ren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                         'foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': [1]*12,\n",
    "                    'D': [1,2,3,4,5,6,7,5,4,3,2,1]})\n",
    " \n",
    "\n",
    "# display(df)\n",
    "\n",
    "grouped = df.groupby(['A', 'B'])\n",
    "grp_sum = grouped.sum()\n",
    "display(grp_sum)\n",
    "\n",
    "# g = grp_sum['D'].groupby(level=0, group_keys=False)\n",
    "# res = g.apply(lambda x: x.order(ascending=False))\n",
    "# res = grp_sum.apply(lambda x: x.sort_values(['D']))\n",
    "res = grp_sum.sort_values(['A', 'C'])\n",
    "display(res)\n",
    "res = grp_sum.sort_values(['A', 'D'])\n",
    "display(res)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                         'foo', 'foo', 'cok', 'cok',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'eight', 'two', 'three',\n",
    "                          'fice', 'six', 'ten', 'three',\n",
    "                          'ten', 'ten', 'eight', 'three'],\n",
    "                    'C': [1]*12,\n",
    "                    'D': range(1,13)})\n",
    "\n",
    "\n",
    "display(df)\n",
    "# flat = df.pivot(index='A')\n",
    "# flat = df.pivot(index='B', columns='B')\n",
    "# flat = df.pivot(index='B', columns=['A'])\n",
    "flat = df.pivot(index='B', columns='A', values='C')\n",
    "\n",
    "# flat = pd.DataFrame(grp_sum.to_records())\n",
    "# flat = grp_sum\n",
    "# display(grp_sum)\n",
    "display(flat)\n",
    "# flat = grp_sum.pivot(columns='A')\n",
    "\n",
    "# display(flat.columns)\n",
    "# display(flat.columns.get_level_values(0))\n",
    "# flat.columns = flat.columns.get_level_values(0)\n",
    "# display(flat)\n",
    "\n",
    "# columns = [' '.join(col).strip() for col in flat.columns.values]\n",
    "# display(columns)\n",
    "\n",
    "# flat = flat.set_axis([f\"{x}{y}\" for x, y in df.columns], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'tot',\n",
    "                         'foo', 'bar', 'foo', 'cho',\n",
    "                          'cho', 'bar', 'lan', 'foo'],\n",
    "                    'C': [1]*12,\n",
    "                    'D': range(1,13)})\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['do', 'bar', 'tot', 'cho'],\n",
    "                    'C': [1]*4,\n",
    "                    'D': range(1,5)})\n",
    "\n",
    "## Inner Join using isin\n",
    "display_side_by_side(df1, df2)\n",
    "join1 = df1[df1['A'].isin(df2['A'])]\n",
    "\n",
    "display(join1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'tot',\n",
    "                         'foo', 'bar', 'foo', 'cho',\n",
    "                          'cho', 'bar', 'lan', 'foo'],\n",
    "                    'C': range(1,3*12,3),\n",
    "                    'D': range(1,13)})\n",
    "\n",
    "df_2 = pd.DataFrame({'A': ['do', 'bar', 'tot', 'cho', 'foo'],\n",
    "                    'C': range(1,3*5,3),\n",
    "                    'D': range(1,6)})\n",
    "df_1.set_index(['A','C'], inplace=True)\n",
    "df_2.set_index(['A','C'], inplace=True)\n",
    "\n",
    "\n",
    "# display(df_1, df_2)\n",
    "display_side_by_side(df_1, df_2)\n",
    "\n",
    "df_merge = pd.merge(df_1, df_2, how='outer', left_index=True, right_index=True)\n",
    "no_nans = df_merge.loc[pd.notnull(df_merge['D_x']) & pd.notnull(df_merge['D_y'])]\n",
    "sumadas = df_merge.sum(axis=1)\n",
    "sumadas = sumadas.to_frame('Suma')\n",
    "# display(df_merge, sumadas)\n",
    "display_side_by_side(df_merge, no_nans, sumadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'tot',\n",
    "                         'foo', 'bar', 'foo', 'cho',\n",
    "                          'cho', 'bar', 'lan', 'foo'],\n",
    "                    'C_1': range(1,3*12,3),\n",
    "                    'D_1': range(1,13)})\n",
    "\n",
    "df_2 = pd.DataFrame({'A': ['lan','do', 'bar', 'tot', 'cho', 'foo'],\n",
    "                    'C_2': range(1,3*6,3),\n",
    "                    'D_2': range(1,7)})\n",
    "\n",
    "df_merge = df_1.merge(df_2, how='outer', on='A', left_index=False, right_index=False)\n",
    "# try how='left' 'right' 'inner'\n",
    "display_side_by_side(df_1, df_2, df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_df = pd.DataFrame({'C': [1]*5,\n",
    "                      'D': range(1,6)})\n",
    "\n",
    "total_row = no_df.sum(numeric_only=True)\n",
    "total_row.name = 'Total'\n",
    "no_df = no_df.append(total_row)\n",
    "# no_df = no_df.append(total_row, ignore_index=True)\n",
    "display(total_row)\n",
    "display(no_df)\n",
    "div_r = no_df.iloc[0:-1]/no_df.iloc[-1]\n",
    "display(div_r)\n",
    "\n",
    "no_df = pd.DataFrame({'C': [1]*5,\n",
    "                      'D': range(1,6),\n",
    "                     'Str': ['uno', 'dos', 'cinco','cabc', 'sor'],\n",
    "                     'bob': ['sss']*5})\n",
    "no_df.set_index('Str', inplace=True)\n",
    "display(no_df)\n",
    "row_sum = no_df.sum(numeric_only=True, axis=1)\n",
    "no_df['Total']=row_sum\n",
    "display(no_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'speed':[1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1]}\n",
    "df = pd.DataFrame(data)\n",
    "wd_size = 5\n",
    "df['MA'] = df['speed'].rolling(window=wd_size).mean()\n",
    "df['MA'].fillna(df['MA'][wd_size-1], inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read CSV with headers and config rows\n",
    "csv_path = '/Users/skulas/Dev/Kin/_GIT_REPOS_/KRE_NOTEBOOKS/KRE/Notebooks/Test_Files/csv_with_config.csv'\n",
    "df_config = pd.read_csv(filepath_or_buffer=csv_path, header=0, nrows=1)\n",
    "display(df_config)\n",
    "conf = df_config.loc[0]\n",
    "# print(dir(pd.Series))\n",
    "print(conf['memo'])\n",
    "print(conf['koko'])\n",
    "\n",
    "df_data = pd.read_csv(filepath_or_buffer=csv_path, header=2)\n",
    "display(df_data)\n",
    "display(df_data.loc[0]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "from xlsxwriter.utility import xl_range\n",
    "\n",
    "df = pd.DataFrame({'Uno': range(1,10), 'Dos': range(2,20,2)})\n",
    "display(df)\n",
    "\n",
    "file_name = '{}dataframe.xlsx'.format(WORKING_FOLDER)\n",
    "print('Saving test excel file {}'.format(file_name))\n",
    "writer = pd.ExcelWriter(file_name, engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='no_shit')\n",
    "\n",
    "\n",
    "## Formulas\n",
    "# file_name = '{}out.xlsx'.format(WORKING_FOLDER)\n",
    "# workbook = xlsxwriter.Workbook(file_name)\n",
    "# worksheet = workbook.add_worksheet('wshit')\n",
    "worksheet = writer.sheets['no_shit']\n",
    "\n",
    "row_num = 0\n",
    "col_num = 1\n",
    "\n",
    "# worksheet.write(row_num, col_num, 'Uno')\n",
    "# worksheet.write(row_num, col_num+1, 'Dos')\n",
    "worksheet.write(row_num, col_num+2, 'Sub Total')\n",
    "row_num+=1\n",
    "for ix, row in df.iterrows():\n",
    "#     worksheet.write(row_num, col_num, row['C_A'])\n",
    "#     worksheet.write(row_num, col_num+1, row['C_B'])\n",
    "    sub_range = xl_range(row_num, col_num, row_num, col_num+1)\n",
    "    worksheet.write(row_num, col_num+2, '=SUM({})'.format(sub_range))\n",
    "    row_num+=1\n",
    "\n",
    "rows, cols = df.shape\n",
    "\n",
    "worksheet.write(row_num, 0, 'Total')\n",
    "sum_range = xl_range(1,3,rows,3)\n",
    "print('Sum range: {}'.format(sum_range))\n",
    "worksheet.write(row_num, 3, '=SUM({})'.format(sum_range))\n",
    "\n",
    "\n",
    "workbook = writer.book\n",
    "worksheet2 = workbook.add_worksheet('yes_shit')\n",
    "row_num = 0\n",
    "col_num = 0\n",
    "worksheet2.write(row_num, col_num, 'Numeros')\n",
    "worksheet2.write(row_num, col_num+1, 'Plus No Shit')\n",
    "row_num+=1\n",
    "for i in range(2,20):\n",
    "    worksheet2.write(row_num, col_num, i)\n",
    "    worksheet2.write(row_num, col_num+1, '=A{}*no_shit!D{}'.format(i, i))\n",
    "    row_num+=1\n",
    "\n",
    "\n",
    "\n",
    "writer.save()\n",
    "\n",
    "\n",
    "# workbook.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Uno': range(1,10), 'Dos': range(2,20,2)})\n",
    "display(df)\n",
    "path = '~/'\n",
    "# Save dataframe to diks\n",
    "df.to_pickle(path + 'test.pkl')\n",
    "\n",
    "# Load dataframe from disk\n",
    "load_df = pd.read_pickle(path + 'test.pkl')\n",
    "display(load_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 format\n",
    "A fast streaming format for large data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_path = '/Users/skulas/Dev/kre_cache_folder/'\n",
    "store = pd.HDFStore(hdf_path + 'store.h5')\n",
    "df1 = pd.DataFrame({'Uno': range(1,10), 'Dos': range(2,20,2)})\n",
    "df2 = pd.DataFrame({'Terra': range(1,5), 'Nova': range(2,10,2)})\n",
    "display_side_by_side(df1, df2)\n",
    "\n",
    "store['df1'] = df1\n",
    "store['df2'] = df2\n",
    "\n",
    "# Load\n",
    "storeL = pd.HDFStore(hdf_path + 'store.h5')\n",
    "df11 = storeL['df1']\n",
    "df22 = storeL['df2']\n",
    "df = storeL['df'] # df was stored upfront and then removed from the code to check it is actually in the file\n",
    "display_side_by_side(df11, df22)\n",
    "display(df)\n",
    "\n",
    "try:\n",
    "    no_existent = storeL['momo']\n",
    "except KeyError:\n",
    "    print(\"Nothing stored for key momo\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                         'foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': [1]*12,\n",
    "                    'D': range(1,13)})\n",
    "\n",
    "mem_sizes = df.memory_usage(deep=True)\n",
    "mem_total = mem_sizes.sum()\n",
    "\n",
    "display(mem_sizes)\n",
    "print(f'{\"*\"*5} Total: {mem_total} {\"*\"*5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Big Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Init Big Query client\n",
    "client = bigquery.Client()\n",
    "\n",
    "wallet_to_search = 'GC2QIF2T7LZLYRGJ7JATB4EHT3MSKVN4ZOWL34REX723H4MM4ZO2F7AL'\n",
    "query_str = \"\"\"\n",
    "            SELECT * FROM `kin-bi.stellar.payments_with_tx_types_view`\n",
    "            WHERE source = '{}' OR\n",
    "            destination = '{}'\n",
    "            \"\"\".format(wallet_to_search, wallet_to_search)\n",
    "\n",
    "# Define Query\n",
    "query_job = client.query(query_str)\n",
    "\n",
    "# Run Query - blocks\n",
    "results = query_job.result()  # Waits for job to complete.\n",
    "\n",
    "# Transform restult to pandas DataFrame\n",
    "res = results.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, func\n",
    "from sqlalchemy import Column, Integer, String, Text\n",
    "from sqlalchemy.orm.attributes import flag_modified\n",
    "from sqlalchemy.ext.mutable import MutableDict\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import JSON, JSONB\n",
    "import json\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "print(f'sqlalchemy version: {sqlalchemy.__version__}' )\n",
    "\n",
    "# postgresql://<username>:<password>@<host>:<port>/<database>\n",
    "# engine = create_engine('postgresql://skulas:@localhost:5432/Playground_DB')\n",
    "engine = create_engine('postgresql://postgres:AA123456@kre-free-pgdb.cpc7e4uruqex.us-east-1.rds.amazonaws.com:5432/playground')\n",
    "meta = sqlalchemy.MetaData(engine) \n",
    "\n",
    "# Default Base class of ORM classes\n",
    "Base = declarative_base()\n",
    "\n",
    "# Session with pgsql server\n",
    "Session = sessionmaker(bind=engine)\n",
    "# Can be done in two steps, for defining the engine later on:\n",
    "# Session = sessionmaker()\n",
    "# ... create engine\n",
    "# Session.configure(bind=engine)\n",
    "\n",
    "session = Session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Mapping - ORM\n",
    "\n",
    "Following this [tutorial](https://docs.sqlalchemy.org/en/13/orm/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    fullname = Column(String)\n",
    "    nickname = Column(String)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<User(name='%s', fullname='%s', nickname='%s')>\" % (\n",
    "        self.name, self.fullname, self.nickname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(User.__table__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create The Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eze_user = User(name='Eze', fullname='Ezequiel Lowi', nickname='skulas')\n",
    "\n",
    "\n",
    "viento = User(name='wendy', fullname='Wendy Williams', nickname='windy')\n",
    "maria = User(name='mary', fullname='Mary Contrary', nickname='mary')\n",
    "pedro = User(name='fred', fullname='Fred Flintstone', nickname='freddy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add(eze_user)\n",
    "session.add_all([viento, maria, pedro])\n",
    "\n",
    "session.new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for instance in session.query(User).order_by(User.id):\n",
    "    print(f'{instance.name}: {instance.fullname}')\n",
    "\n",
    "print('*'*20)\n",
    "\n",
    "for name, fullname in session.query(User.name, User.fullname).order_by(User.name):\n",
    "    print(f'{name}: {fullname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(User).filter(User.name.in_(['fred', 'Eze'])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suckers = session.query(User).filter(User.name.in_(['wendy', 'Eze'])).all()\n",
    "\n",
    "session.delete(suckers[0])\n",
    "session.delete(suckers[1])\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Docs\n",
    "\n",
    "Following this [tutorial](https://www.compose.com/articles/using-json-extensions-in-postgresql-from-python-2/)<br>\n",
    "And this tutorial leads to this [notebook](https://gist.github.com/ryansb/3ad9b7ccf225d46a16dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Table with JSON Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with json documents\n",
    "sqlalchemy.Table(\"jsontable\", meta,\n",
    "                Column('id', Integer, primary_key=True),\n",
    "                Column('name', Text),\n",
    "                Column('email', Text),\n",
    "                Column('doc', JSON))\n",
    "meta.create_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a table object\n",
    "j_table = sqlalchemy.table(\"jsontable\",\n",
    "                           Column('id', Integer),\n",
    "                           Column('name', Text),\n",
    "                           Column('email', Text),\n",
    "                           Column('doc', JSON))\n",
    "# drop existing records\n",
    "engine.execute(j_table.delete())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a record to the table with a raw query string\n",
    "\n",
    "engine.execute(\"\"\"INSERT INTO jsontable VALUES (1, 'Ezequiel', 'eze@qui.el', '{\"dialect\": \"raw\", \"numero\": 1234}')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a record using params\n",
    "\n",
    "statement = j_table.insert().values(\n",
    "        id=3,\n",
    "        name=\"Mr. Params\",\n",
    "        email=\"use@params.com\",\n",
    "        doc={\n",
    "            \"dialect\": \"params\",\n",
    "            \"address\": {\"street\": \"Main St.\", \"zip\": 12345},\n",
    "            \"numero\": 65432\n",
    "        },\n",
    "    )\n",
    "engine.execute(statement)\n",
    "print(str(statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the DB\n",
    "find_user = j_table.select().where(j_table.c.name == \"Mr. Params\")\n",
    "user_found = engine.execute(find_user).fetchone()\n",
    "print(f'Found user: {user_found}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select by querying inside the json document\n",
    "find_zip = j_table.select().where(j_table.c.doc[('address', 'zip')].astext.cast(sqlalchemy.Integer) == 12345)\n",
    "\n",
    "engine.execute(find_zip).fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * * * * * * ROLLBACK * * * * * * *  * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORM with JSON Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unmutable Version, the JSON cannot be updated\n",
    "class User(Base):  \n",
    "    __tablename__ = 'jsontable'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(Text)\n",
    "    email = Column(Text)\n",
    "    doc = Column(JSON)\n",
    "    \n",
    "Base.metadata.create_all(engine)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "u = User( \n",
    "    id=5,\n",
    "    name=\"Oren ORM\",\n",
    "    email=\"oren@orms.com\",\n",
    "    doc={\"address\": {\"zip\": 5566, \"street\": \"Sistic St.\"}, \"numero\":7754})\n",
    "session.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "random_string = lambda N: ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(N))\n",
    "print(random_string(12))\n",
    "\n",
    "# Batch insert\n",
    "id_n = 1001\n",
    "for i in range(0,500):\n",
    "     u = User( \n",
    "          id=id_n,\n",
    "          name=f\"Dotad{random_string(4)}\",\n",
    "          email=f\"dot{random_string(2)}a@aa{random_string(3)}.co.il\",\n",
    "          doc={\"address\": {\"zip\": random.randint(1,10000), \"street\": f\"sre{random_string(3)}le{random_string(3)}\"}, \"numero\":random.randint(1,1000)})\n",
    "     session.add(u) \n",
    "     id_n+=1\n",
    "\n",
    "print('START COMMIT')\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uu = session.query(User).filter(User.id == 5).one()\n",
    "print(f'{uu.id} - {uu.name}')\n",
    "print(uu.doc)\n",
    "\n",
    "uu = session.query(User).filter(User.doc[('address', 'zip')].astext.cast(sqlalchemy.Integer) > 7078).first()\n",
    "res = session.query(User).filter(User.doc[('address', 'zip')].astext.cast(sqlalchemy.Integer) > 5000).count()\n",
    "\n",
    "print(f'{uu.id} - {uu.name}')\n",
    "print(uu.doc)\n",
    "\n",
    "print(f'Count = {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Mutable version of User (the JSON doc is mutable)\n",
    "class MUser(Base):  \n",
    "    __tablename__ = 'jsontable'\n",
    "    __table_args__ = {'extend_existing': True} \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(Text)\n",
    "    email = Column(Text)\n",
    "    doc = Column(MutableDict.as_mutable(JSON))\n",
    "    \n",
    "Base.metadata.create_all(engine)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_2_search = '0jr'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a mutable extension, or even defining user.doc as a mutable dictionary didn't help."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "uu = session.query(MUser).filter(MUser.name.like(f\"%{str_2_search}%\")).first()\n",
    "\n",
    "if uu:\n",
    "    print(type(uu))\n",
    "    print(f'{uu.id} - {uu.name}')\n",
    "    uu.name = 'Alex xel'\n",
    "    \n",
    "    newDoc = dict(uu.doc)\n",
    "    newDoc['address']['street'] = 'Santiago del Estereo'\n",
    "    uu.doc = newDoc\n",
    "    print(uu.doc)\n",
    "\n",
    "#     uu.doc['address']['street'] = 'Santiago del Estereo'\n",
    "    session.commit()\n",
    "else:\n",
    "    print(f'No users with name containing {str_2_search} were found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu = session.query(User).filter(User.name.like(f\"%{str_2_search}%\")).first()\n",
    "\n",
    "if uu:\n",
    "    print(f'{uu.id} - {uu.name}')\n",
    "    uu.name = 'Agaga dada'\n",
    "    \n",
    "    uu.doc['address']['street'] = 'Upid St.'\n",
    "    print(uu.doc)\n",
    "    \n",
    "    flag_modified(uu, \"doc\")\n",
    "# Not needed    session.add(uu)\n",
    "\n",
    "    session.commit()\n",
    "else:\n",
    "    print(f'No users with name containing {str_2_search} were found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation\n",
    "\n",
    "[Example 1](https://gist.github.com/anddam/ff7cb849825ad76f85e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "res = session.query(User).filter(User.doc[('address', 'zip')].astext.cast(sqlalchemy.Integer) > 5000).first()\n",
    "print(res.name)\n",
    "print('*'*40)\n",
    "# db.session.query(TestModel, func.count(TestModel.id)).group_by(TestModel.name).all()                         \n",
    "# res = session.query(User, func.sum(User.doc['numero'].astext.cast(sqlalchemy.Integer))).scalar() # .filter(User.doc[('address', 'zip')].astext.cast(sqlalchemy.Integer) > 5000)\n",
    "res = session.query(User, func.sum(User.doc['numero'].astext.cast(sqlalchemy.Integer))).scalar() # .filter(User.doc[('address', 'zip')].astext.cast(sqlalchemy.Integer) > 5000)\n",
    "res = session.query.group_by(User.id)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_2_search = 'dada'\n",
    "\n",
    "uu = session.query(User).filter(User.name.like(f\"%{str_2_search}%\")).first()\n",
    "\n",
    "if uu:\n",
    "    print(f'Deleting user: {uu.id} - {uu.name}')\n",
    "    session.delete(uu)\n",
    "    session.commit()\n",
    "else:\n",
    "    print(f'No users with name containing {str_2_search} were found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggreate JSON docs fields - Query String\n",
    "Following this [tutorial](http://www.postgresqltutorial.com/postgresql-json/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_zip = 1803\n",
    "\n",
    "query_str = f\"\"\"\n",
    "select sum(cast(doc->>'numero' as integer)) as numero_sum from jsontable\n",
    "where cast(doc->'address'->>'zip' as integer) > {max_zip}\n",
    "\"\"\"\n",
    "connection = engine.connect()\n",
    "result = connection.execute(query_str)\n",
    "print(f'The total sum of numeros for zip above {max_zip} is {result.first()[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into pd df\n",
    "min_amount = 15\n",
    "query_str = \"\"\"\n",
    "                SELECT * FROM public.\"WALLET_GC2Q_ACTIVITY\" \n",
    "                WHERE amount > {}\n",
    "                ORDER BY amount ASC;\n",
    "               \"\"\".format(min_amount)\n",
    "dfmin = pd.read_sql_query(query_str, con=engine)\n",
    "display(dfmin.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows in tableB\n",
    "query_str = \"\"\"\n",
    "                SELECT COUNT(*) FROM public.\"WALLET_GC2Q_ACTIVITY\" \n",
    "                WHERE amount > {}\n",
    "               \"\"\".format(min_amount)\n",
    "\n",
    "connection = engine.connect()\n",
    "result = connection.execute(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Result methods')\n",
    "print(dir(result))\n",
    "print(result.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Date Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "\n",
    "now = dt.datetime.now()\n",
    "now_str = now.strftime('%d/%m/%Y')\n",
    "print('Today is {}'.format(now_str))\n",
    "print(f'Now time: {now.strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "print(dt.datetime(2019, 2, 29))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of days in month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "print('Number of days in 2012/1:', calendar.monthrange(2012,1)[1])\n",
    "print('Number of days in 2012/2:', calendar.monthrange(2012,2)[1])\n",
    "print('Number of days in 2000/2:', calendar.monthrange(2000,2)[1])\n",
    "print('Number of days in 1900/2:', calendar.monthrange(1900,2)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DateUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "sample_date = dt.datetime.strptime('2018-09-01', '%Y-%m-%d')\n",
    "print(sample_date)\n",
    "next_date = sample_date + relativedelta(months=+1)\n",
    "print(next_date)\n",
    "sd = dt.datetime(next_date.year, next_date.month, next_date.day)\n",
    "next_date = sd + relativedelta(months=+1)\n",
    "print(next_date)\n",
    "next_date = next_date + relativedelta(months=+1)\n",
    "next_date = next_date + relativedelta(months=+1)\n",
    "print(next_date)\n",
    "print(sd < next_date)\n",
    "print(next_date < sd)\n",
    "print(f'This is the default date format {next_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeZones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone as tz\n",
    "from datetime import datetime as dt\n",
    "tzinfo=tz.utc\n",
    "utcdate_01_Oct_1974_16_30_utc = dt(1974, 10, 1, 16, 30, tzinfo=tzinfo)\n",
    "print(utcdate_01_Oct_1974_16_30_utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame datetime64[ns, UTC] column to epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refdatetime = dt.datetime(1970,1,1, tzinfo=None)\n",
    "refdatetime = dt.datetime(1970,1,1, tzinfo=tz.utc)\n",
    "#     refdatetime = dt.datetime(1970,1,1, tzinfo=None)\n",
    "#     refdatetime = dt.datetime(1970,1,1, tzinfo=tz.utc)\n",
    "\n",
    "#     testtime = df_renamed['tx_t'].iloc[0]\n",
    "#     print(testtime)\n",
    "#     print(testtime.tzinfo)\n",
    "#     print(refdatetime)\n",
    "#     print(refdatetime.tzinfo)\n",
    "#     print(df_renamed['tx_t'].shape)\n",
    "\n",
    "#     lista = df_renamed['tx_t'].tolist()\n",
    "#     %timeit (lista - refdatetime).dt.total_seconds()\n",
    "    \n",
    "#     %timeit (df_renamed['tx_t'] - refdatetime).dt.total_seconds()\n",
    "#     %timeit df_renamed['tx_t'].astype('int64')//1e9\n",
    "\n",
    "(df_renamed['tx_t'] - refdatetime).dt.total_seconds()\n",
    "df_renamed['tx_t'].astype('int64')//1e9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "later = dt(2019,5,30)\n",
    "earlier = dt(2019,5,5)\n",
    "days_between = (later - earlier).days\n",
    "print(f'Number of days between {later} and {earlier} is {days_between}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default value for string\n",
    "a = None\n",
    "b = ''\n",
    "c = a or 'default value for c, as a is None'\n",
    "d = b or 'default value for d, as b is None'\n",
    "print('c = {} and d = {}'.format(c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '29'\n",
    "month = '2'\n",
    "year = '2017'\n",
    "\n",
    "date = '{}-{}-{}'.format(year, month, day)\n",
    "try:\n",
    "    dt.datetime.strptime(date, '%Y-%m-%d')\n",
    "except ValueError as verror:\n",
    "    print(\n",
    "        'Failure parsing date day:{}, month:{}, year:{}. {}'.\n",
    "        format(day, month, year, verror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add / only if needed\n",
    "import os\n",
    "\n",
    "# Mading sure path ends with /\n",
    "path1 = '/Users/tito/gomez'\n",
    "path = os.path.join(path1, '')\n",
    "print(path)\n",
    "\n",
    "path2 = '/Users/momo/lopqez/'\n",
    "path = os.path.join(path2, '')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "random_string = lambda N: ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(N))\n",
    "print(random_string(12))\n",
    "print(random_string(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Float: {0:0.2f}'.format(313.1415))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['un', 'lindo', 'dia']\n",
    "lista = [f\"\"\"'{x}'\"\"\" for x in lista]\n",
    "this_str = f\"\"\"\n",
    "This is an array:'{','.join(lista)}'\"\"\"\n",
    "print(this_str)\n",
    "\n",
    "query_str = f\"\"\"\n",
    "        SELECT date\n",
    "            ,digital_service_name\n",
    "            ,digital_service_id\n",
    "            ,COUNT(*) AS total_spend_transactions\n",
    "            ,SUM(amount) AS total_spend_transaction_volume_in_kin\n",
    "            ,COUNT(distinct user_wallet_id) number_of_distinct_spend_wallets\n",
    "        FROM `kin-bi.stellar.payments_with_tx_types_view`\n",
    "        WHERE EXTRACT(MONTH FROM date) = {3}\n",
    "        AND EXTRACT(YEAR FROM date) = {2019}\n",
    "        AND tx_type in ({','.join(lista)})\n",
    "        AND digital_service_name not in ('Kinit','other','anonymous', 'TestApp')\n",
    "        GROUP BY date, digital_service_id,digital_service_name\n",
    "        ORDER BY date, total_spend_transactions DESC\n",
    "\"\"\"\n",
    "print(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Int, bool, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = True\n",
    "print(f'True is {xxx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1,5)\n",
    "lx = list(x)\n",
    "y1 = [2*i for i in lx]\n",
    "y2 = lx\n",
    "y3 = [0.5*i for i in lx]\n",
    "ptitle = 'Stack'\n",
    "fig = plt.figure(ptitle, figsize=(9,4))\n",
    "plt.plot(x,y1, label='2x')\n",
    "plt.title(ptitle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptitle = 'Stack'\n",
    "fig = plt.figure(ptitle, figsize=(9,4))\n",
    "plt.stackplot(x,[y1, y2, y3], labels=['2x','x', '0.5x'])\n",
    "plt.legend(loc='best')\n",
    "plt.title(ptitle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptitle = 'Other Stack'\n",
    "fig = plt.figure(ptitle, figsize=(9,4))\n",
    "plt.stackplot(x,[y3, y2, y1], labels=['0.5x','x', '2x'])\n",
    "plt.legend(loc='best')\n",
    "plt.title(ptitle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism and Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Execution\n",
    "Launch processes to run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Process, Pool\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1():\n",
    "    print('f1 start')\n",
    "    for i in range(1,10):\n",
    "        print(f'f1-{i}')\n",
    "        time.sleep(0.2)\n",
    "            \n",
    "    print('f1 is done')\n",
    "\n",
    "def f2():\n",
    "    print('f2 start')\n",
    "    for i in range(11,20):\n",
    "        print(f'f2-{i}')\n",
    "        time.sleep(0.25)\n",
    "            \n",
    "    print('f2 is done')\n",
    "\n",
    "def f3():\n",
    "    print('f3 start')\n",
    "    for i in range(111,120):\n",
    "        print(f'f3-{i}')\n",
    "        time.sleep(0.35)\n",
    "            \n",
    "    print('f3 is done')\n",
    "\n",
    "p1 = Process(target=f1)\n",
    "p2 = Process(target=f2)\n",
    "p3 = Process(target=f3)\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "p3.start()\n",
    "\n",
    "p1.join() # lock until p1 is done\n",
    "print('p1 exit')\n",
    "p2.join()\n",
    "print('p2 exit')\n",
    "p3.join()\n",
    "print('p3 exit')\n",
    "\n",
    "print(' --- all done --- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=2) as pp:\n",
    "    pp.apply_async(func=f1)\n",
    "    pp.apply_async(func=f2)\n",
    "    pp.apply_async(func=f3)    \n",
    "\n",
    "    pp.close()\n",
    "    pp.join()\n",
    "\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent\n",
    "Execute code in diffrent **threads** in the same process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1():\n",
    "    print('f1 start')\n",
    "    for i in range(1,10):\n",
    "        print(f'f1-{i}')\n",
    "        time.sleep(0.2)\n",
    "            \n",
    "    print('f1 is done')\n",
    "\n",
    "def f2():\n",
    "    print('f2 start')\n",
    "    for i in range(11,20):\n",
    "        print(f'f2-{i}')\n",
    "        time.sleep(0.25)\n",
    "            \n",
    "    print('f2 is done')\n",
    "\n",
    "def f3():\n",
    "    print('f3 start')\n",
    "    for i in range(111,120):\n",
    "        print(f'f3-{i}')\n",
    "        time.sleep(0.35)\n",
    "            \n",
    "    print('f3 is done')\n",
    "\n",
    "def run_func(f):\n",
    "    f()\n",
    "\n",
    "arr = [f1, f2, f3]\n",
    "\n",
    "with PoolExecutor(max_workers=4) as executor:\n",
    "    print('Start')\n",
    "    for _ in executor.map(run_func, arr):\n",
    "        pass # Wailts till all executor threads are done running\n",
    "    \n",
    "    print('All Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_arr(arr):\n",
    "    arr_len = len(arr)\n",
    "    print(f'Printing {arr_len} items')\n",
    "    sleep_time = 2 / arr_len\n",
    "    for i in arr:\n",
    "        print(f'{i}-')\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "a1 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "a2 = [11,11,11,11,11,11,11]\n",
    "a3 = [111,111,111,111,111,111,111,111,111,111,111,111,111]\n",
    "a4 = [1111,1111,1111,1111,1111,1111,1111,1111,1111,1111,1111,1111]\n",
    "a5 = [2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]\n",
    "a6 = [22,22,22,22,22,22,22,22,22,22,22,22,22,22]\n",
    "\n",
    "a = [a1, a2, a3, a4, a5, a6]\n",
    "with PoolExecutor(max_workers=4) as executor:\n",
    "    print('Start')\n",
    "    for _ in executor.map(print_arr, a):\n",
    "        pass # Wailts till all executor threads are done running\n",
    "    \n",
    "    print('All Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_strs(str1, str2, str3, str4, st):\n",
    "    time.sleep(st)\n",
    "    print(str1)\n",
    "    time.sleep(st)\n",
    "    print(str2)\n",
    "    time.sleep(st)\n",
    "    print(str3)\n",
    "    time.sleep(st)\n",
    "    print(str4)\n",
    "    return(f'Input: {str1}, {str2}, {str3}, {str4}, {st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_tups_arr = [(\"uno\", \"dos\", \"tres\", 0.1),\n",
    "                (\"trolo\", \"loco\", \"super\", 0.125),\n",
    "                (\"arena\", \"papaya\", \"thailand\", 0.15),\n",
    "                (\"tren\", \"auto\", \"modto\", 0.178)]\n",
    "args_tups_arr = ((a, b, c, 'fin', t) for a,b,c,t in args_tups_arr)\n",
    "\n",
    "with PoolExecutor(max_workers=4) as executor:\n",
    "    for result in executor.map(lambda p: print_strs(*p), args_tups_arr):   # (*p) does the unpacking part\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coroutines, asyncio, await\n",
    "[A nice more elaborated example](https://medium.com/python-pandemonium/asyncio-coroutine-patterns-beyond-await-a6121486656f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "def print_now():\n",
    "    now = dt.datetime.now()\n",
    "    now_str = now.strftime('%d/%m/%Y')\n",
    "    print(f'Time: {now.strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "async def await_sleep(w=0, s=1, doaw=True):\n",
    "    if doaw:\n",
    "        await asyncio.sleep(w)\n",
    "    else:\n",
    "        asyncio.sleep(w)\n",
    "    print_now()\n",
    "    await asyncio.sleep(s)\n",
    "    print_now()\n",
    "    return f'resultado de await_sleep s={s}, w={w}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcallback(t):\n",
    "    print(f'Result: {t.result()}')\n",
    "    print(f't.done= {t.done()}') \n",
    "\n",
    "def gathercallback(g):\n",
    "    print('Gathering is over')\n",
    "    display(type(g))\n",
    "#     display(dir(g)) \n",
    "    print('RESULT:')\n",
    "    display(g.result())\n",
    "    display(f'gathering done: {g.done()}')\n",
    "\n",
    "def sync_main():\n",
    "    print_now()\n",
    "    print('-'*8)\n",
    "\n",
    "    task1 = asyncio.create_task(await_sleep(1, 5))\n",
    "    task1.add_done_callback(tcallback)\n",
    "    task2 = asyncio.create_task(await_sleep(2, 2))\n",
    "    task2.add_done_callback(tcallback)\n",
    "    print(f't.done= {task1.done()}')  \n",
    "          \n",
    "    task3 = asyncio.create_task(await_sleep(3, 4))\n",
    "    res = asyncio.gather(task3, await_sleep(4, 3), loop=None, return_exceptions=False)\n",
    "    res.add_done_callback(gathercallback)\n",
    "#     print(dir(res))\n",
    "    \n",
    "#     res = asyncio.wait_for(await_sleep(4, 5), 20)\n",
    "#     loop = asyncio.get_event_loop()\n",
    "#     loop = asyncio.get_running_loop()\n",
    "#     res = asyncio.wait_for(await_sleep(4, 5), 20)\n",
    "#     loop = asyncio.new_event_loop()\n",
    "#     res = loop.run_until_complete(await_sleep(4, 5))\n",
    "#     print(res)\n",
    "\n",
    "sync_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('asyncio.run')\n",
    "print_now()\n",
    "print('-'*8)\n",
    "res = asyncio.run(await_sleep(3, 3))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asyncio in Multi-threaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from threading import Thread\n",
    "import datetime as dt\n",
    "\n",
    "def start_loop(loop, tasks=None, flag='basa'):\n",
    "    what_to_do = create_tasks(flag=flag)\n",
    "#     what_to_do = []\n",
    "#     for t in tasks:\n",
    "#         what_to_do.append(asyncio.ensure_future(t))\n",
    "#     new_loop = asyncio.new_event_loop()\n",
    "\n",
    "    asyncio.set_event_loop(loop)\n",
    "    results = loop.run_until_complete(asyncio.gather(*what_to_do))\n",
    "#     results = loop.run_until_complete(create_tasks(flag=flag))\n",
    "    rix = 0\n",
    "    for res in results:\n",
    "        rix += 1\n",
    "        print(f'{rix}. {res}')\n",
    "#     loop.run_forever()\n",
    "\n",
    "async def print_async(flag='no_flag', sleep_period=1):\n",
    "    await asyncio.sleep(sleep_period)\n",
    "    now = dt.datetime.now()\n",
    "    now_str = now.strftime('%d/%m/%Y')\n",
    "    print(f'{now.strftime(\"%H:%M:%S\")} - {flag} done waiting {sleep_period}')\n",
    "    return(f'{flag} {sleep_period}')\n",
    "    \n",
    "# def create_tasks(flag='t_d_f'):\n",
    "#     # Create parallel tasks\n",
    "#     tasks = []\n",
    "#     tasks.append(asyncio.create_task(print_async(flag, 2)))\n",
    "#     tasks.append(asyncio.create_task(print_async(flag, 1)))\n",
    "#     tasks.append(asyncio.create_task(print_async(flag, 4)))\n",
    "    \n",
    "#     return tasks\n",
    "\n",
    "# def create_tasks(flag='t_d_f'):\n",
    "#     # Create parallel tasks\n",
    "#     asyncio.create_task(print_async(flag, 2))\n",
    "#     asyncio.create_task(print_async(flag, 1))\n",
    "#     asyncio.create_task(print_async(flag, 4))\n",
    "    \n",
    "#     return tasks\n",
    "\n",
    "def create_tasks(flag='t_d_f'):\n",
    "    # Create parallel tasks\n",
    "    tasks = []\n",
    "    tasks.append(print_async(flag, 2))\n",
    "    tasks.append(print_async(flag, 1))\n",
    "    tasks.append(print_async(flag, 4))\n",
    "    \n",
    "    return tasks\n",
    "         \n",
    "def kick_new_thread(flag='thread_flag'):\n",
    "#     tasks = create_tasks(flag=flag)\n",
    "    \n",
    "    new_loop = asyncio.new_event_loop()\n",
    "    t = Thread(target=start_loop, args=(new_loop, None, flag, ))\n",
    "#     t = Thread(target=start_loop, args=(loop=new_loop, tasks=tasks, flag=flag, ))\n",
    "    t.start()\n",
    "#     new_loop.run_until_complete(asyncio.gather(*tasks))\n",
    "    \n",
    "kick_new_thread('Batman')\n",
    "kick_new_thread('Robin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML and AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "e**X**treme **G**radient **Boost**ing\n",
    "XGBoost [settings](https://xgboost.readthedocs.io/en/latest/parameter.html).<br>\n",
    "Parameter tunning in XGBoost: [Great tutorial](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) with links to deeper learning on Boosting and XGBusting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "iris = datasets.load_iris()\n",
    "wine = datasets.load_wine()\n",
    "# print(dir(datasets))\n",
    "# print(dir(iris))\n",
    "# print(iris.filename)\n",
    "print(f'type of wine: {type(wine)}')\n",
    "print(f'type of iris: {type(iris)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "X = wine.data\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test / train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'type(X_train) = {type(X_train)}')\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'Y_train.shape = {y_train.shape}')\n",
    "ys = set(y_train)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for XGBoost to be able to use our data, we’ll need to transform it into a specific format that XGBoost can handle. That format is called DMatrix. It’s a very simple one-linear to transform a numpy array of data to DMatrix format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "print(f'type(D_train) = {type(D_train)}')\n",
    "# dir(xgb.DMatrix)\n",
    "print(f'feature names: {D_train.feature_names}')\n",
    "print(f'feature types: {D_train.feature_types}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, # Complicated, read explanation below this block\n",
    "    'max_depth': 4,  # maximum depth of the decision trees being trained (number of leaves of each tree)\n",
    "    'objective': 'multi:softprob',  # calculates multi class probabilities\n",
    "    'num_class': 3} # the number of classes in the dataset (three related species in the iris case)\n",
    "\n",
    "steps = 20  # The number of training iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ETA** Param\n",
    "<p>From our theory, Gradient Boosting involves creating and adding decision trees to an ensemble model sequentially. New trees are created to correct the residual errors in the predictions from the existing ensemble.\n",
    "Due to the nature of an ensemble, i.e having several models put together to form what is essentially a very large complicated one, makes this technique prone to overfitting. The <b>eta</b> parameter gives us a chance to prevent this overfitting.</p>\n",
    "<p>The eta can be thought of more intuitively as a learning rate. Rather than simply adding the predictions of new trees to the ensemble with full weight, the eta will be multiplied by the residuals being adding to reduce their weight. This effectively reduces the complexity of the overall model.</p>\n",
    "<p>It is common to have small values in the range of 0.1 to 0.3. The smaller weighting of these residuals will still help us train a powerful model, but won’t let that model run away into deep complexity where overfitting is more likely to happen.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)\n",
    "# model.dump_model()\n",
    "print(model.attributes())\n",
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the results\n",
    "\n",
    "<p>\n",
    "    Every classifier can be evaluated as a dichotomous (yes/no) desicion on each class.<br>\n",
    "    For each class we can then define a confusion matrix.\n",
    "</p>\n",
    "<p>\n",
    "    True Possitive Rate<br>\n",
    "    TPR = tp/(tp+fn) = tp/p<br>\n",
    "    AKA: Recall, Sensitivity\n",
    "</p>\n",
    "<p>\n",
    "    False Possitive Rate<br>\n",
    "    FPR = fp/(fp+tn) = fp/n\n",
    "</p>\n",
    "<p>\n",
    "    Presicion<br>\n",
    "    PPV = tp/(tp+fp)\n",
    "</p>\n",
    "<p>\n",
    "    Accuracy<br>\n",
    "    Acuracy = (tp + tn) /(tp + fp + tn + fn)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Some interesting parameters worth listing. For the [entire list](https://xgboost.readthedocs.io/en/latest/parameter.html) visit XGBoost site.\n",
    "\n",
    "**eta**: limit the learning rate of the model (to avoid overfitting)<br>\n",
    "**max_depth**: maximum depth of the decision trees being trained (log(number of leaves) of each tree) <br>\n",
    "**objective**: the loss function being used<br>\n",
    "**num_class**: the number of classes in the dataset. Number of different labels to chose from.<br>\n",
    "**gamma** : can also help with controlling overfitting. It specifies the minimum reduction in the loss required to make a further partition on a leaf node of the tree. I.e if creating a new node doesn’t reduce the loss by a certain amount, then we won’t create it at all.<br>\n",
    "**booster** : allows you to set the type of model you will use when building the ensemble. The default is gbtree which builds an ensemble of decision trees. If your data isn’t too complicated, you can go with the faster and simpler gblinear option which builds an ensemble of linear models.<br>\n",
    "\n",
    "**GRID Search** the hyperparams to optimizing our model<br>\n",
    "[Good guide](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) aboud parameter optimization of XGBoost in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "##########\n",
    "# from sklearn import metrics\n",
    "# # this is the direct xgboost library. Has a specific function “cv”: cross-validation\n",
    "# import xgboost as xgb \n",
    "# # this is an sklearn wrapper for XGBoost. This allows us to use sklearn’s Grid Search with parallel processing\n",
    "# from xgboost.sklearn import XGBClassifier \n",
    "\n",
    "# def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "#     if useTrainCV:\n",
    "#         xgb_param = alg.get_xgb_params()\n",
    "#         xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "#         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "#             metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "#         alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "#     #Fit the algorithm on the data\n",
    "#     alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "        \n",
    "#     #Predict training set:\n",
    "#     dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "#     dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "#     #Print model report:\n",
    "#     print(\"\\nModel Report\")\n",
    "#     print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "#     print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "###########\n",
    "\n",
    "\n",
    "# display(iris)\n",
    "clf = XGBClassifier() # The sklearn wrapper\n",
    "\n",
    "#split data\n",
    "\n",
    "print(len(y_test))\n",
    "X_train_pd = pd.DataFrame(X_train)\n",
    "y_train_pd = pd.Series(y_train)\n",
    "# print(type(y_train))\n",
    "\n",
    "# display(X_train)\n",
    "#set up xgboost with grid search and k-fold cv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "        \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "        'max_depth': [1, 2, 3, 4, 5, 6],\n",
    "        'gamma': [0.0, 0.001, 0.01, 0.1],\n",
    "        'missing': [0.0, 1.0],\n",
    "        'n_estimators': [1, 2, 3, 4, 5, 6, 10, 20],\n",
    "#         \"gamma\"       : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "#         'learning_rate': [0.001, 0.01, 0.1, 0.2, 1],\n",
    "                }\n",
    "\n",
    "# clf = XGBClassifier()\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=8, n_jobs=6)\n",
    "# grid_search.fit(X_train_df, y_train)\n",
    "grid_search.fit(X_train_pd, y_train_pd)\n",
    "\n",
    "print ('Best parameters:', grid_search.best_params_)\n",
    "print ('Best estimator:', grid_search.best_estimator_)\n",
    "print ('Best score:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est = grid_search.best_estimator_\n",
    "display(best_est.get_xgb_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb2\n",
    "\n",
    "\n",
    "param = best_est.get_xgb_params()\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['num_class'] = 3\n",
    "\n",
    "\n",
    "steps = 20  # The number of training iterations\n",
    "model2 = xgb2.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model2.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(f\"Precision = {precision_score(y_test, best_preds, average='macro')}\")\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
