{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions and Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions and Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTIVITIES\n",
    "\n",
    " - Move KIN between a closed group of wallets _with_ or _without_ the DS wallets among the group\n",
    "   - Automated\n",
    "   - Manually\n",
    " - Perform failed transactions in purpose\n",
    " - Create exhagerated number of wallets<br>Generates many unique active spenders from same user\n",
    " - Wallet paying to itself\n",
    " - Create wallet without a DS as the wallet creator\n",
    " - Use false app-id to do fraudulent activities\n",
    " - Perform earn/spend activities without user aknowledge (transactions without experiences)\n",
    " - **DS FRAUD** towards her own users. DS can still money from users (the DS holds the user private key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBS\n",
    " \n",
    " - Transactions\n",
    "     - KIN DAS vs Apptopia DAU<br>Check if they internaly onboard existing users gradually into KIN<br>\n",
    "       Deviation from a utility function between active users and daily spends\n",
    "     - Failed / Success ratio\n",
    "     - Spend / Earn ratio\n",
    "     - Variance on tx type and amount\n",
    "     - Set an euristic initial growth ratio that if exceeded we become red.\n",
    "     - Users or Wallets / Transactions (same, initial threshold for red detection)\n",
    "     - Distribution of users according to the ratio S/E. Number of users that spend half of what they earned, 1/4, 0.9, etc\n",
    " - Time\n",
    "     - Bursts of transactions\n",
    "     - Constant / Similar time spacing between transactions\n",
    " - Wallets\n",
    "     - Wallets vs Apptopia MAU<br>\n",
    "       The number of new wallets should be some reasonable portion of the active users of the DS.<br>\n",
    "       This ratio can be calculated out the DS themselves.<br>\n",
    "       Set an initial wallets growth of 5% (euristic choice) above a critical number of existing wallets. If this 5% is 100-1000 wallets, relatively low volatility.<br>\n",
    "       Update this 5% threshold as we learn more about the actual behaviour of the ecosystem<br>\n",
    "       Chose different time frames for measuring metrics. Daily / every three days / weekley / e t c\n",
    "     - Wallets creation growth over time<br>\n",
    "       Could be detected as a setup for the beginning of an automated trading between false wallets.\n",
    "     - Wallets number vs tx or uDAS count\n",
    "     - Active wallets. Wallet aging.\n",
    "     - Sleepy wallet sudden wakeup\n",
    "     - P2P oriented\n",
    "         - Detect closed wallet loops\n",
    "         - Cliques\n",
    "         - Ping-pong\n",
    "         - Black Hole (a single wallet getting paid by lots). Track funds path afterwards.\n",
    "             - All funds transferred to DS or other SINGLE wallet\n",
    "             - Funds stay there\n",
    "             - Funds are spread\n",
    "         - Wallet Age/first transaction time. Comare across P2P wallets and detect trading wallets of same age.\n",
    "         - Do daily analysis\n",
    "         - Compare between days to find recurrent groups of same wallets\n",
    "     - _Deprecated_: Use `kin-bi.devprog.events_view` to detect wallet creation created by robot or not (Variance on carrier, model, location, etc...)\n",
    " - **SDK Data**<br>\n",
    "    **Make this Clear**: We need to get out of the blockchain. To get information that inherently is not available in the blockchain.\n",
    "    We don’t need the data, but detect the activity is performed by legitimate users and not robots or other kinds of users performing activities to influence our results.\n",
    "\n",
    "    - Location\n",
    "        - Geographical distribution of new wallets\n",
    "        - Geographical distribution of transactions\n",
    "    - Environmental\n",
    "        - **Cookies** are useful for identifying we are in the same browser / device (in the case many accounts are open from the same computer, for example)\n",
    "        - **IP** wallets distribution\n",
    "        - **Client type** (browser type, device os, game engine …) wallet creator\n",
    "    - KYC\n",
    "        - Email and Phone can be super helpful\n",
    " - Methods\n",
    "      - Create a pivot table in excel and detect anomalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, probes are implemented by data extraction and analysis. Each probe is first implemented in this Notebook and once ready it can be implemented in the fraud detection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Begin With"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Correlation between Apptopia Active Users and KIN activity (WALLET CREATION)\n",
    " - Utility function between wallets number and number of spends. (WALLET CREATION / TX)\n",
    " - Compare with SDK data reporting and detect that the events are legit - MUST CHECK THIS DATA IS USEFUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probes Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. High Frequency Fluctuations - Done\n",
    "1. Low Frequency Fluctuations - Done\n",
    "1. Cliques Detection - Starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probes:\n",
    "- Frequency Fluctuations:  The red alerts table focuses the user to check the graph and validate if activity shall be further investigated or not.\n",
    "- Apptopia Validaton: Manually check activity of red alerts of apps and compare if activity in KIN makes sense given the DAU readings in Apptopia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run probes<br>\n",
    "Green - All good<br>\n",
    "Red - If we found something bad - not green - go manually and check why it is red\n",
    "The Red at the beginning is less accurate (many false positives) so we improve the criteria and make each criterion more accurate.<br>\n",
    "Generate an output file with the reds table. An xls file would be a good base for future features.\n",
    "\n",
    "The moment we detect a sure Red, we contact the DS to understand together the activity. If it’s too bad, then we also prepare an explanation of why this is wrong and how to behave from now on.\n",
    "It is very important to detect the soonest possible.\n",
    "\n",
    "Tell Orad to pay attention when he talks to DSs about how they measure themselves when they discuss the KRE with him, so we learn the way they think. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Fraud Research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Data Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For pretty printing\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "#feature extraction - Uncomment next three imports when restarting feature analysis\n",
    "# from tsfresh import extract_features\n",
    "# Feature cleanup irrelevants\n",
    "# from tsfresh import select_features\n",
    "# from tsfresh.utilities.dataframe_functions import impute \n",
    "\n",
    "# CONSTS\n",
    "DATA_PATH = '/Users/skulas/Dev/Kin/Data/pickles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def today_to_string(sep_char = '/'):\n",
    "    now = dt.datetime.now()\n",
    "    return now.strftime('%d{}%m{}%Y'.format(sep_char, sep_char))\n",
    "\n",
    "# Encode transaction type\n",
    "#  Earn  -> 4\n",
    "#  Spend -> 2\n",
    "#  P2P   -> 1\n",
    "def encode_transaction_type(tx_history=None):\n",
    "    if tx_history is None:\n",
    "        return None\n",
    "    mapping = {'p2p': 1, 'spend': 2, 'earn': 4}\n",
    "    new_pd = tx_history.replace({'tx_type': mapping})\n",
    "    \n",
    "    return new_pd\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Large Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "\n",
    "import numpy.lib\n",
    "import numpy as np\n",
    "# import cPickle as pickle\n",
    "\n",
    "def save_pandas(fname, data):\n",
    "    '''Save DataFrame or Series\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        filename to use\n",
    "    data: Pandas DataFrame or Series\n",
    "    '''\n",
    "    np.save(open(fname, 'w'), data)\n",
    "    if len(data.shape) == 2:\n",
    "        meta = data.index,data.columns\n",
    "    elif len(data.shape) == 1:\n",
    "        meta = (data.index,)\n",
    "    else:\n",
    "        raise ValueError('save_pandas: Cannot save this type')\n",
    "    s = pickle.dumps(meta)\n",
    "    s = s.encode('string_escape')\n",
    "    with open(fname, 'a') as f:\n",
    "        f.seek(0, 2)\n",
    "        f.write(s)\n",
    "\n",
    "def load_pandas(fname, mmap_mode='r'):\n",
    "    '''Load DataFrame or Series\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        filename\n",
    "    mmap_mode : str, optional\n",
    "        Same as numpy.load option\n",
    "    '''\n",
    "    values = np.load(fname, mmap_mode=mmap_mode)\n",
    "    with open(fname) as f:\n",
    "        numpy.lib.format.read_magic(f)\n",
    "        numpy.lib.format.read_array_header_1_0(f)\n",
    "        f.seek(values.dtype.alignment*values.size, 1)\n",
    "        meta = pickle.loads(f.readline().decode('string_escape'))\n",
    "    if len(meta) == 2:\n",
    "        return pd.DataFrame(values, index=meta[0], columns=meta[1])\n",
    "    elif len(meta) == 1:\n",
    "        return pd.Series(values, index=meta[0])\n",
    "\n",
    "file_path = \"pkl.pkl\"\n",
    "def write_huge_pkl(data, file_path):\n",
    "#     n_bytes = 2**31\n",
    "    max_bytes = 2**31 - 1\n",
    "#     data = bytearray(n_bytes)\n",
    "\n",
    "    ## write\n",
    "    bytes_out = pickle.dumps(data)\n",
    "    with open(file_path, 'wb') as f_out:\n",
    "        for idx in range(0, len(bytes_out), max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "\n",
    "def read_huge_pkl(file_path):\n",
    "    max_bytes = 2**31 - 1\n",
    "\n",
    "    ## read\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(file_path)\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "    data = pickle.loads(bytes_in)\n",
    "    return data\n",
    "\n",
    "    assert(data == data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Methods\n",
    "\n",
    "def fetch_list_of_all_devs(force_refresh = False):\n",
    "    all_devs_path = DATA_PATH + 'all_devs'\n",
    "\n",
    "    try:\n",
    "        if force_refresh:\n",
    "            raise FileNotFoundError('Should refresh data from Database, ignoring file on disk')\n",
    "        all_devs_list = pd.read_pickle(all_devs_path)\n",
    "        print('Loaded all devs list from local disk.')\n",
    "    except FileNotFoundError:\n",
    "        print('Fetching all devs list from DB')\n",
    "        query_str = 'SELECT * FROM `kin-bi.kin.digital_services_lookup_view`'\n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(query_str)\n",
    "\n",
    "        results = query_job.result()  # Waits for job to complete.\n",
    "        all_devs_list = results.to_dataframe()\n",
    "        all_devs_list.to_pickle(all_devs_path)\n",
    "        print('List of all devs saved to disk')\n",
    "    \n",
    "    return all_devs_list\n",
    "\n",
    "    \n",
    "def get_dev_by_wallet(devs_list=None, wallet=None):\n",
    "    if wallet is None:\n",
    "        return None\n",
    "\n",
    "    if devs_list is None:\n",
    "        devs_list = fetch_list_of_all_devs()\n",
    "    \n",
    "    our_guy = devs_list[(devs_list['app_wallet_sender'] == wallet) |\n",
    "                        (devs_list['app_wallet_recipient'] == wallet) |\n",
    "                        (devs_list['app_wallet_sender_2'] == wallet) |\n",
    "                        (devs_list['app_wallet_recipient_2'] == wallet)\n",
    "                       ]\n",
    "    return our_guy\n",
    "\n",
    "def get_dev_by_id(dev_id, devs_list=None):\n",
    "    if dev_id is None:\n",
    "        return None\n",
    "    \n",
    "    if devs_list is None:\n",
    "        devs_list = fetch_list_of_all_devs()\n",
    "    \n",
    "    our_guy = devs_list[(devs_list['digital_service_id'] == dev_id)]\n",
    "    return our_guy\n",
    "\n",
    "def read_tx_history_from_DB(query_str):\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query_str)\n",
    "\n",
    "    results = query_job.result()  # Waits for job to complete.\n",
    "    res = results.to_dataframe()\n",
    "    print(query_str)\n",
    "\n",
    "    return res\n",
    "\n",
    "def read_tx_history_from_DB_by_wallet(dev_object, base_date):\n",
    "    DEFAULT = 'XX_NO_WALLET'\n",
    "    w1 = dev_object.app_wallet_sender.iloc[0] or 'NO_app_wallet_sender'\n",
    "    w2 = dev_object.app_wallet_recipient.iloc[0] or 'NO_app_wallet_recipient'\n",
    "    w3 = dev_object.app_wallet_sender_2.iloc[0] or 'NO_app_wallet_sender_2'\n",
    "    w4 = dev_object.app_wallet_recipient_2.iloc[0] or 'NO_app_wallet_recipient_2'\n",
    "    wallets_str = \"'{}', '{}', '{}', '{}'\".format(w1, w2, w3, w4)\n",
    "\n",
    "    query_str = \"\"\"\n",
    "        SELECT * FROM `kin-bi.stellar.payments_with_tx_types_view` \n",
    "        WHERE date >= '{}'\n",
    "        AND (source in ({}) or \n",
    "        destination in ({}))\n",
    "        ORDER BY date\n",
    "    \"\"\".format(base_date, wallets_str, wallets_str)\n",
    "\n",
    "    tx_history = read_tx_history_from_DB(query_str)\n",
    "    return tx_history\n",
    "\n",
    "def read_tx_history_from_DB_by_devid(dev_object, base_date):\n",
    "    devid = dev_object.digital_service_id.iloc[0]\n",
    "\n",
    "    query_str = \"\"\"\n",
    "        SELECT * FROM `kin-bi.stellar.payments_with_tx_types_view` \n",
    "        WHERE date >= '{}'\n",
    "        AND digital_service_id = '{}'\n",
    "        ORDER BY date\n",
    "    \"\"\".format(base_date, devid)\n",
    "\n",
    "    tx_history = read_tx_history_from_DB(query_str)\n",
    "    return tx_history\n",
    "\n",
    "# NOTE: Using kin-bi.kre.payments_with_tx_types_view_with_status which is a frozen view from 1-Jan till 20-Feb 2019\n",
    "def get_trans_history(base_date='2018-12-20'):\n",
    "    today_history_path = '{}all_tx_history_since_{}.csv'.format(DATA_PATH, \n",
    "                                                            today_to_string('_'))\n",
    "    \n",
    "    try:\n",
    "        tx_history = pd.read_pickle(today_history_path)\n",
    "        print('Loaded all tx history from disk')\n",
    "    except FileNotFoundError:\n",
    "        query_str = \"\"\"\n",
    "                    SELECT * FROM `kin-bi.kre.payments_with_tx_types_view_with_status` \n",
    "                    WHERE date >= '{}'\n",
    "                \"\"\".format(base_date)\n",
    "        \n",
    "        tx_history = read_tx_history_from_DB(query_str)\n",
    "        \n",
    "        encoded_trans = encode_transaction_type(tx_history)\n",
    "        tx_history['coded_tx_type'] = encoded_trans['tx_type']\n",
    "\n",
    "    return tx_history\n",
    "\n",
    "def save_data_to_pikle(name_prefix='all_tx_history_since', df_to_save=None):\n",
    "    if df_to_save is None:\n",
    "        return\n",
    "\n",
    "    today_history_path = '{}{}_{}.csv'.format(DATA_PATH,\n",
    "                                              name_prefix,\n",
    "                                              today_to_string('_'))\n",
    "\n",
    "    print('Savin {} to {}'.format(name_prefix, today_history_path))\n",
    "    df_to_save.to_pickle(today_history_path)\n",
    "        \n",
    "\n",
    "    \n",
    "def get_dev_trans_history(dev_object, \n",
    "                          day=None,\n",
    "                          month=None, \n",
    "                          year=None, \n",
    "                          base_date='2018-12-20',\n",
    "                          search_param='wallet'):\n",
    "    \"\"\"\n",
    "    Build a list of transactions since base_date\n",
    "    Attempt to get it from the disk (saved daily)\n",
    "    :param dev_object: a df row of the dev we want to search history\n",
    "    :param day, month, year: The date a file has been saved to the disk (a search date)\n",
    "    :param base_date: The date which the transaction history shall begin from.\n",
    "    :param search_param: Search by wallet or by ds id? 'id' or 'wallet'\n",
    "    \"\"\"\n",
    "    \n",
    "    if dev_object is None:\n",
    "        return None\n",
    "    \n",
    "    dev_id = dev_object.digital_service_id.iloc[0]\n",
    "#     today_history_path = DATA_PATH + 'tx_history_for_kinit_' + dev_id + '_' + today_to_string('_')\n",
    "    today_history_path = '{}tx_history_by_{}_for_{}_{}'.format(DATA_PATH, \n",
    "                                                                  search_param,\n",
    "                                                                  dev_id,\n",
    "                                                                  today_to_string('_'))\n",
    "    \n",
    "    if day is None or month is None or year is None:\n",
    "        tx_history_path = today_history_path\n",
    "    else:\n",
    "        # Read a file alredy available in memory\n",
    "        tx_history_path = '{}tx_history_by_{}_for_{}_{}_{}_{}'.format(DATA_PATH, \n",
    "                                                                     search_param,\n",
    "                                                                     dev_id,\n",
    "                                                                     day,\n",
    "                                                                     month,\n",
    "                                                                     year)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        tx_history = pd.read_pickle(tx_history_path)\n",
    "        print('Loaded tx history from disk')\n",
    "    except FileNotFoundError:\n",
    "        if search_param == 'id':\n",
    "            \n",
    "            tx_history = read_tx_history_from_DB_by_devid(dev_object, base_date)\n",
    "        elif search_param == 'wallet':\n",
    "            tx_history = read_tx_history_from_DB_by_wallet(dev_object, base_date)\n",
    "        else:\n",
    "            return None\n",
    "        encoded_trans = encode_transaction_type(tx_history)\n",
    "        tx_history['coded_tx_type'] = encoded_trans['tx_type']\n",
    "        tx_history.to_pickle(today_history_path)\n",
    "        print('Saved tx history to disk')\n",
    "\n",
    "    return tx_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate testing of methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "devs_list = fetch_list_of_all_devs()\n",
    "some_dev = get_dev_by_id('yqyf', devs_list)\n",
    "display('wallet = {}'.format(some_dev.app_wallet_recipient.iloc[0]))\n",
    "display(devs_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "devtest = get_dev_by_wallet(all_devs_list, 'GDGVVFQDIDRHEE2RSV4E7ENXMJYVAIVWCQFSPCU7IMTJAQPZ6VNACLLB')\n",
    "display(devtest)\n",
    "historia = get_dev_trans_history(dev_object=devtest, day='06', month='02', year='2019', base_date='2019-01-13')\n",
    "historia = get_dev_trans_history(dev_object=devtest, base_date='2019-01-13', search_param='id')\n",
    "# historia = get_dev_trans_history(dev_object=devtest, base_date='2019-01-13')\n",
    "display(historia.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_devs_list = fetch_list_of_all_devs()\n",
    "display(all_devs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_devs_list.iloc[0])\n",
    "display(all_devs_list.iloc[0]['app_wallet_sender'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Run Query and save results into disk in a pickled pandas\n",
    "# If pandas exists, just load it from disk\n",
    "# Don't execute this cell, call the function instead. Kept cell for reading dev_program only, in case I'd need it.\n",
    "dev_program_devs_path = DATA_PATH + 'dev_program_devs'\n",
    "all_devs_path = DATA_PATH + 'all_devs'\n",
    "\n",
    "try:\n",
    "    devs_list = pd.read_pickle(dev_program_devs_path)\n",
    "    print('Loaded devs list from local disk.')\n",
    "except FileNotFoundError:\n",
    "    query_str = 'SELECT * FROM `kin-bi.devprog.kdp_participants`'\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query_str)\n",
    "\n",
    "    results = query_job.result()  # Waits for job to complete.\n",
    "    devs_list = results.to_dataframe()\n",
    "    devs_list.to_pickle(dev_program_devs_path)\n",
    "    print('List of devs saved to disk')\n",
    "\n",
    "try:\n",
    "    all_devs_list = pd.read_pickle(all_devs_path)\n",
    "    print('Loaded all devs list from local disk.')\n",
    "except FileNotFoundError:\n",
    "    query_str = 'SELECT * FROM `kin-bi.kin.digital_services_lookup_view`'\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query_str)\n",
    "\n",
    "    results = query_job.result()  # Waits for job to complete.\n",
    "    all_devs_list = results.to_dataframe()\n",
    "    all_devs_list.to_pickle(all_devs_path)\n",
    "    print('List of all devs saved to disk')\n",
    "\n",
    "display(all_devs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study - Kiny\n",
    "An employee within one of the Developers members of the developers program ([Kinit](https://kinexplorer.com/statistics)) performed false actions during tests he was making on the blockchain. These activity has direct impact on the KRE payment this developoer would be granted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALLET = 'GBY5PZFDZ6Y25S6YRRZ3CXOAIUWOZ3ADONFY2OYCA7GPQCPPF2RDXXZC'\n",
    "DEV_ID = 'rced'\n",
    "our_guy = get_dev_by_wallet(all_devs_list, WALLET)\n",
    "display(our_guy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_history = get_dev_trans_history(dev_object=our_guy)\n",
    "number_of_rows, _ = tx_history.shape\n",
    "print('Since 25/Dec found {} transactions'.format(number_of_rows))\n",
    "display(tx_history.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "daily_histogram_data = tx_history['date'].astype(\"datetime64\")\n",
    "sfig = plt.figure('Tx Histogram', figsize=(16,9))\n",
    "daily_histogram = daily_histogram_data.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns = {'amount': 'sum'}\n",
    "volume_df = tx_history.groupby(tx_history['date']).aggregate(interesting_columns)\n",
    "sfig = plt.figure('Daily Volume', figsize=(16,9))\n",
    "splt = plt.plot(volume_df.index, volume_df['amount'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All transactions done by developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows, _ = tx_history.shape\n",
    "print('Since 25/Dec found {} transactions of our guy with one of his wallets'.format(number_of_rows))\n",
    "tx_history = get_dev_trans_history(dev_object=our_guy, search_param='id')\n",
    "number_of_rows, _ = tx_history.shape\n",
    "print('Since 25/Dec found {} transactions of our guy with any wallet.'.format(number_of_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_histogram_data = tx_history['amount']\n",
    "# unique_ammounts = daily_histogram_data.drop_duplicates()\n",
    "sfig = plt.figure('Tx Histogram', figsize=(16,9))\n",
    "daily_histogram = daily_histogram_data.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Extraction Experiment\n",
    "Try extracting features and see what comes out.<br>\n",
    "**Paused it, will get back to feature extraction later on**<br>\n",
    "To enable these cells convert them back from raw to code by selecting each (not in edit mode) and typing ```y```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_to_analyze = tx_history[['time', 'tx_hashhash', 'amount', 'coded_tx_type]]\n",
    "extracted_features = extract_features(data_to_analyze, column_id=\"tx_hash\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extracted_features.shape\n",
    "impute(extracted_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract indices of transactions with the most common ammount and of transactions that amount > 80% of MAX(Tx.amount)\n",
    "filtered_features = select_features(extracted_features, data_to_analyze['amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interesting Values\n",
    "Extract information about transactions for the interesting developer<br>\n",
    "##### Tx Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "amounts = tx_history['amount']\n",
    "max_ammount = amounts.max()\n",
    "min_ammount = amounts.min()\n",
    "avg_amount = amounts.mean()\n",
    "median_amount = amounts.median()\n",
    "print('Max: {}\\nMin: {}\\nMedian: {}\\nAvg: {}'.format(max_ammount, min_ammount, median_amount, avg_amount))\n",
    "uniques_count = pd.DataFrame(tx_history.groupby('amount')['tx_hash'].nunique())\n",
    "display(uniques_count.head(10))\n",
    "display(uniques_count.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Developers Analysis\n",
    "Learn behavioural patterns of digital service providers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://skulas:@localhost:5432/KRE_Fraud_Data')\n",
    "sorted_tx_history.to_sql('tx_history', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the postgress server:\n",
    "(we could run it from here, but then we need to be sure the server is up before running anything)<br>\n",
    "`pg_ctl -D /usr/local/var/postgres start`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Too big for pickle save_data_to_pikle(name_prefix='all_tx_history_since', df_to_save=sorted_tx_history)\n",
    "# If want to save \n",
    "sorted_tx_history.memory_usage(deep=True)/(1024*1024)\n",
    "/Users/skulas/Dev"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Too big for  msg pack  sorted_tx_history.to_msgpack('/Users/skulas/Dev/test.msgpk')\n",
    "save_pandas('/Users/skulas/Dev/test.pkl', sorted_tx_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Devs List"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# implemented in FIRST GEN, thus disabled here\n",
    "all_devs_list = fetch_list_of_all_devs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(all_devs_list)\n",
    "# display(sorted_tx_history.head(1))\n",
    "print(all_devs_list[all_devs_list['digital_service_id']=='p365']['digital_service_name'].iloc[0])\n",
    "# for index, row in all_devs_list.iterrows():\n",
    "#     display(Markdown('## {}'.format(row.digital_service_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Report Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "ron_pwd = os.environ['RON_PG_PWD']\n",
    "ron_engine = create_engine(f'postgresql://postgres:{ron_pwd}@history-collector.cmmqkby1prsb.us-east-1.rds.amazonaws.com:5432/kin')\n",
    "\n",
    "PG_KIN2_USERNAME = os.environ['PG_KIN_2_USER']\n",
    "PG_KIN2_PWD = os.environ['PG_KIN_2_PWD']\n",
    "PG_KIN2_CONNSTR = 'before-migration-blockchain-history.cywwhhj0pzz8.us-east-1.rds.amazonaws.com:5432/kin'\n",
    "PG_KIN3_USERNAME = os.environ['PG_KIN_3_USER']\n",
    "PG_KIN3_PWD = os.environ['PG_KIN_3_PWD']\n",
    "PG_KIN3_CONNSTR = 'kin-blockchain-history.cywwhhj0pzz8.us-east-1.rds.amazonaws.com:5432/kin'\n",
    "\n",
    "KIN2_ENGINE = create_engine(f'postgresql://{PG_KIN2_USERNAME}:{PG_KIN2_PWD}@{PG_KIN2_CONNSTR}')\n",
    "KIN3_ENGINE = create_engine(f'postgresql://{PG_KIN3_USERNAME}:{PG_KIN3_PWD}@{PG_KIN3_CONNSTR}')\n",
    "\n",
    "\n",
    "def run_query_on_postgres(query_str, engine=ron_engine):\n",
    "    dataframe = pd.read_sql_query(query_str, con=engine)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def run_query_on_hc(query_str, engine=KIN2_ENGINE):\n",
    "    dataframe = pd.read_sql_query(query_str, con=engine)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def get_wallets_of_developer(dev_id):\n",
    "    dev_wallets_df = all_devs_list[all_devs_list['digital_service_id']==dev_id][['app_wallet_sender',\n",
    "                                                                                'app_wallet_recipient',\n",
    "                                                                                'app_wallet_sender_2',\n",
    "                                                                                'app_wallet_recipient_2'\n",
    "                                                                                ]]\n",
    "    dev_wallets_list = dev_wallets_df.values.tolist()\n",
    "    dev_wallets_list = list(filter(bool, dev_wallets_list[0]))\n",
    "\n",
    "    return dev_wallets_list\n",
    "\n",
    "def sql_strlist_from_list(str_list):\n",
    "    lstr = \"','\".join(str_list)\n",
    "    sqlstr = f\"('{lstr}')\"\n",
    "    \n",
    "    return sqlstr\n",
    "\n",
    "def get_wallets_open_by_develoer_by_date(dev_id, run_on_kin3=False):\n",
    "    query_str = f\"\"\"\n",
    "                    SELECT time::date as date, COUNT(destination) as number_of_wallets\n",
    "                    FROM public.creations\n",
    "                    WHERE memo_text = '1-{dev_id}'\n",
    "                    GROUP BY time::date\n",
    "                    ORDER BY time::date ASC\n",
    "    \"\"\"\n",
    "\n",
    "#     results_df = run_query_on_postgres(query_str)\n",
    "    if run_on_kin3:\n",
    "        results_df = run_query_on_hc(query_str, KIN3_ENGINE)\n",
    "    else:\n",
    "        results_df = run_query_on_hc(query_str, KIN2_ENGINE)\n",
    "        \n",
    "    results_df.set_index('date', inplace=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def fetch_tx_history_for_dev(dev_id):\n",
    "    query_str = f\"\"\"\n",
    "        SELECT * FROM `kin-bi.stellar.payments_with_tx_types_view` \n",
    "        WHERE digital_service_id = '{dev_id}'\n",
    "        ORDER BY date\n",
    "    \"\"\"\n",
    "    dev_history = read_tx_history_from_DB(query_str)\n",
    "    \n",
    "    # Encode tx_type\n",
    "    encoded_trans = encode_transaction_type(dev_history)\n",
    "    dev_history['coded_tx_type'] = encoded_trans['tx_type']\n",
    "    \n",
    "    return dev_history\n",
    "\n",
    "def fetch_udas_history_for_dev(dev_id):\n",
    "        query_str = f\"\"\"\n",
    "            SELECT date, COUNT(DISTINCT(source)) as unique_spenders\n",
    "            FROM `kin-bi.stellar.payments_with_tx_types_view` \n",
    "            WHERE tx_type in ('spend','p2p')\n",
    "            AND digital_service_id = '{dev_id}'\n",
    "            GROUP BY date\n",
    "            ORDER BY date ASC\n",
    "        \"\"\"\n",
    "        udas = read_tx_history_from_DB(query_str)\n",
    "        return udas\n",
    "        \n",
    "def draw_wallts_vs_tx(data):\n",
    "    avg_ratio = 1.5\n",
    "    fig = plt.figure('New wallets vs # of tx', figsize=(16,9))\n",
    "#     fig.suptitle(\"New wallets vs # of tx\", fontsize=16)\n",
    "    ax = plt.subplot(\"211\")\n",
    "    ax.set_title(\"New Walltes\")\n",
    "    ax.plot(data.index, data['number_of_wallets'], label=\"Wallets Creation\")\n",
    "    mean_now = data['number_of_wallets'].mean()\n",
    "    ax.set_ylim([0, avg_ratio*mean_now])\n",
    "\n",
    "    ax = plt.subplot(\"212\")\n",
    "    ax.set_title(\"# of Transactions\")\n",
    "    ax.plot(data.index, data['number_of_tx'], label=\"Transactions Count\")\n",
    "    mean_not = data['number_of_tx'].mean()\n",
    "    ax.set_ylim([0, avg_ratio*mean_not])\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "def generate_report(dev_id):\n",
    "    dev_history = fetch_tx_history_for_dev(dev_id)\n",
    "\n",
    "    number_of_rows, _ = dev_history.shape\n",
    "    dev_name = all_devs_list[all_devs_list['digital_service_id']==dev_id]['digital_service_name'].iloc[0]\n",
    "    display(Markdown('## {}'.format(dev_name)))\n",
    "    print(f'Found {number_of_rows} transactions for dev id: {dev_id}.')\n",
    "    \n",
    "    # Transactions Histogram - better use the\n",
    "    #     daily_histogram_data = dev_history['date'].astype(\"datetime64\")\n",
    "    #     sfig = plt.figure('Tx Histogram', figsize=(16,9))\n",
    "    #     daily_histogram = daily_histogram_data.hist(bins=100)\n",
    "    #     daily_histogram.title.set_text('Transactions Histogram')\n",
    "\n",
    "    # uDAS \n",
    "    daily_udas = fetch_udas_history_for_dev(dev_id)\n",
    "    # udas_histogram_data = daily_udas['date'].astype(\"datetime64\")\n",
    "    sfig = plt.figure('uDAS', figsize=(16,9))\n",
    "    plt.plot(daily_udas['date'], daily_udas['unique_spenders'])\n",
    "    plt.title('uDAS')\n",
    "\n",
    "    # Wallets vs Transactions for dates\n",
    "    sub_data = dev_history[['date', 'tx_hash']]\n",
    "    sub_data = sub_data.rename(columns={'tx_hash': 'number_of_tx'})\n",
    "    wallets_history = get_wallets_open_by_develoer_by_date(dev_id)\n",
    "    groups = sub_data.groupby(['date'])\n",
    "    counts = groups.count()\n",
    "    counts = counts.sort_values(['date'])\n",
    "    together = pd.merge(wallets_history, counts, how='outer', on='date')\n",
    "    together = together.sort_values(['date'])\n",
    "    together.fillna(0, inplace=True)\n",
    "    draw_wallts_vs_tx(together)\n",
    "   \n",
    "    \n",
    "    # Ammount Histogram\n",
    "    daily_histogram_data = dev_history['amount']\n",
    "    sfig = plt.figure('Am Histogram', figsize=(16,9))\n",
    "    daily_histogram = daily_histogram_data.hist(bins=200)\n",
    "    daily_histogram.title.set_text('Amount Histogram')\n",
    "    \n",
    "    # Transactions Type Histogram\n",
    "    daily_histogram_data = dev_history['coded_tx_type']\n",
    "    sfig = plt.figure('Tx Type Histogram', figsize=(16,9))\n",
    "    daily_histogram = daily_histogram_data.hist(bins=6)\n",
    "    daily_histogram.title.set_text('Type Histogram     1 -> p2p     2 -> spend     4 -> earn')\n",
    "    \n",
    "    # Transactions and Ammounts\n",
    "    sub_data = dev_history[['tx_type', 'amount', 'date']]\n",
    "    sub_data = sub_data.rename(columns={'date': 'Count'})\n",
    "    groups = sub_data.groupby(['tx_type', 'amount'])\n",
    "    counts = groups.count()\n",
    "    counts = counts.sort_values(['tx_type', 'Count'], ascending=False)\n",
    "    display(counts)\n",
    "    \n",
    "    return together\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud Detect - FIRST GEN PROBES\n",
    "Develop modules as they'll be used in the automated fraud detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRE_FT_PATH = '/Users/skulas/Dev/kre_cache_folder/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "\n",
    "def date_str_from_ymd(year, month, day, sep_char='-'):\n",
    "    date_str = f'{year}{sep_char}{month}{sep_char}{day}'\n",
    "    try:\n",
    "        dt.datetime.strptime(date_str, f'%Y{sep_char}%m{sep_char}%d')\n",
    "        return date_str\n",
    "    except ValueError as verror:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BIG QUERY INTERFACE\n",
    "\n",
    "def run_on_bq(query_str):\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query_str)\n",
    "\n",
    "    results = query_job.result()\n",
    "    res = results.to_dataframe()\n",
    "\n",
    "    return res\n",
    "\n",
    "def fetch_dev_history_by_id_till(dev_id, end_date_year, end_date_month, end_date_day):\n",
    "    date_str = date_str_from_ymd(end_date_year, end_date_month, end_date_day)\n",
    "    \n",
    "    if date_str is None:\n",
    "        print(f'ERROR WITH DATE {end_date_year}, {end_date_month}, {end_date_day}')\n",
    "        return None\n",
    "    \n",
    "    query_str = f\"\"\"\n",
    "        SELECT * FROM `kin-bi.stellar.payments_with_tx_types_view` \n",
    "        WHERE digital_service_id = '{dev_id}'\n",
    "        AND date <= '{date_str}'\n",
    "        ORDER BY time ASC\n",
    "    \"\"\"\n",
    "    dev_history = run_on_bq(query_str)\n",
    "    return dev_history\n",
    "\n",
    "def fetch_dev_history_by_from_till(dev_id,\n",
    "                                   start_date_year, start_date_month, start_date_day,\n",
    "                                   end_date_year, end_date_month, end_date_day):\n",
    "    \"\"\"\n",
    "    NOTE:\n",
    "    Search is done in half open range: start_date <= Search < end_date.\n",
    "    Meaning end_date should be the first day of the next time span.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_date_str = date_str_from_ymd(start_date_year, start_date_month, start_date_day)\n",
    "    if start_date_str is None:\n",
    "        print(f'ERROR WITH DATE {start_date_year}, {start_date_month}, {start_date_day}')\n",
    "        return None\n",
    "    \n",
    "    end_date_str = date_str_from_ymd(end_date_year, end_date_month, end_date_day)\n",
    "    if end_date_str is None:\n",
    "        print(f'ERROR WITH DATE {end_date_year}, {end_date_month}, {end_date_day}')\n",
    "        return None\n",
    "    \n",
    "    start_dt = dt.datetime(start_date_year, start_date_month, start_date_day)\n",
    "    end_dt = dt.datetime(end_date_year, end_date_month, end_date_day)\n",
    "    if end_dt < start_dt:\n",
    "        print(f'ERROR WITH DATES: (end){end_date_str} < {start_date_str}(start)')\n",
    "        return None\n",
    "    \n",
    "    query_str = f\"\"\"\n",
    "        SELECT * FROM `kin-bi.stellar.payments_with_tx_types_view` \n",
    "        WHERE digital_service_id = '{dev_id}'\n",
    "        AND '{start_date_str}' <= date\n",
    "        AND date < '{end_date_str}'\n",
    "        ORDER BY time ASC\n",
    "    \"\"\"\n",
    "    dev_history = run_on_bq(query_str)\n",
    "    return dev_history\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abstract Data Layer\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "\n",
    "def get_devs_list():\n",
    "    # refresh the devs list once a day\n",
    "    today_str = today_to_string(sep_char = '-')\n",
    "    devs_file_name = f'devs_list_{today_str}.pkl'\n",
    "    devs_file_path = f'{KRE_FT_PATH}{devs_file_name}'\n",
    "    df_devs = None\n",
    "    try:\n",
    "        df_devs = pd.read_pickle(path=devs_file_path)\n",
    "    except FileNotFoundError:\n",
    "        df_devs = fetch_list_of_all_devs(force_refresh = True)\n",
    "        df_devs.to_pickle(path=devs_file_path)\n",
    "    \n",
    "    return df_devs\n",
    "\n",
    "def dev_name_by_id(dev_id):\n",
    "    all_devs_list = get_devs_list()\n",
    "    our_dev = all_devs_list[all_devs_list['digital_service_id']==dev_id]\n",
    "    if our_dev.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return our_dev['digital_service_name'].iloc[0]\n",
    "    \n",
    "def get_dev_history_by_id(dev_id, end_date_year, end_date_month, end_date_day):\n",
    "    date_str = date_str_from_ymd(end_date_year, end_date_month, end_date_day)\n",
    "    if date_str is None:\n",
    "        print('ERROR *** ERROR')\n",
    "        return None\n",
    "    \n",
    "    file_name = f'dev_history_{dev_id}_{date_str}.pkl'\n",
    "    file_path = f'{KRE_FT_PATH}{file_name}'\n",
    "    df_dev_history = None\n",
    "    try:\n",
    "        df_dev_history = pd.read_pickle(path=file_path)\n",
    "    except FileNotFoundError:\n",
    "        df_dev_history = fetch_dev_history_by_id_till(dev_id, end_date_year, end_date_month, end_date_day)\n",
    "        # QUERY data sorted df_dev_history = df_dev_history.sort_values(by=['time'], axis=0)\n",
    "        try:\n",
    "            df_dev_history.to_pickle(path=file_path)\n",
    "        except FileNotFoundError:\n",
    "            print('FileNotFoundError: Data too big to be saved to pickle')\n",
    "        except:\n",
    "            print('GENERIC ERROR: Failed to write data to pickle')\n",
    "    \n",
    "    return df_dev_history\n",
    "\n",
    "def get_monthly_segments(end_date_year, end_date_month, end_date_day):\n",
    "    end_date_str = date_str_from_ymd(end_date_year, end_date_month, end_date_day)\n",
    "    if end_date_str is None:\n",
    "        print(f'ERROR WITH DATE {end_date_year}, {end_date_month}, {end_date_day}')\n",
    "        return None\n",
    "    \n",
    "    START_DATE = dt.datetime(2018,9,1)\n",
    "    dates = []\n",
    "    curr_date = START_DATE\n",
    "    next_date = curr_date + relativedelta(months=+1)\n",
    "    end_date = dt.datetime(end_date_year, end_date_month, end_date_day)\n",
    "    \n",
    "    while next_date <= end_date:\n",
    "        args_tup = (curr_date.year, curr_date.month, curr_date.day,\n",
    "                   next_date.year, next_date.month, next_date.day)\n",
    "        dates.append(args_tup)\n",
    "        curr_date = next_date\n",
    "        next_date = curr_date + relativedelta(months=+1)\n",
    "    return dates\n",
    "\n",
    "def get_weekly_segments(start_date_year, start_date_month, start_date_day,\n",
    "                        end_date_year, end_date_month, end_date_day):\n",
    "\n",
    "    start_date_str = date_str_from_ymd(start_date_year, start_date_month, start_date_day)\n",
    "    if start_date_str is None:\n",
    "        print(f'ERROR WITH DATE {start_date_year}, {start_date_month}, {start_date_day}')\n",
    "        return None\n",
    "    \n",
    "    end_date_str = date_str_from_ymd(end_date_year, end_date_month, end_date_day)\n",
    "    if end_date_str is None:\n",
    "        print(f'ERROR WITH DATE {end_date_year}, {end_date_month}, {end_date_day}')\n",
    "        return None\n",
    "    \n",
    "    start_date = dt.datetime(start_date_year, start_date_month, start_date_day)\n",
    "    dates = []\n",
    "    curr_date = start_date\n",
    "    next_date = curr_date + relativedelta(weeks=+1)\n",
    "    end_date = dt.datetime(end_date_year, end_date_month, end_date_day)\n",
    "    \n",
    "    while next_date <= end_date:\n",
    "        args_tup = (curr_date.year, curr_date.month, curr_date.day,\n",
    "                   next_date.year, next_date.month, next_date.day)\n",
    "        dates.append(args_tup)\n",
    "        curr_date = next_date\n",
    "        next_date = curr_date + relativedelta(weeks=+1)\n",
    "    return dates\n",
    "\n",
    "def get_n_days_segments(start_date_year, start_date_month, start_date_day,\n",
    "                        end_date_year, end_date_month, end_date_day, number_of_days=5):\n",
    "\n",
    "    start_date_str = date_str_from_ymd(start_date_year, start_date_month, start_date_day)\n",
    "    if start_date_str is None:\n",
    "        print(f'ERROR WITH DATE {start_date_year}, {start_date_month}, {start_date_day}')\n",
    "        return None\n",
    "    \n",
    "    end_date_str = date_str_from_ymd(end_date_year, end_date_month, end_date_day)\n",
    "    if end_date_str is None:\n",
    "        print(f'ERROR WITH DATE {end_date_year}, {end_date_month}, {end_date_day}')\n",
    "        return None\n",
    "    \n",
    "    start_date = dt.datetime(start_date_year, start_date_month, start_date_day)\n",
    "    dates = []\n",
    "    curr_date = start_date\n",
    "    next_date = curr_date + relativedelta(days=+number_of_days)\n",
    "    end_date = dt.datetime(end_date_year, end_date_month, end_date_day)\n",
    "    \n",
    "    while next_date <= end_date:\n",
    "        args_tup = (curr_date.year, curr_date.month, curr_date.day,\n",
    "                   next_date.year, next_date.month, next_date.day)\n",
    "        dates.append(args_tup)\n",
    "        curr_date = next_date\n",
    "        next_date = curr_date + relativedelta(days=+number_of_days)\n",
    "    return dates\n",
    "\n",
    "def get_dev_history_by_from_till(dev_id,\n",
    "                                 start_date_year, start_date_month, start_date_day,\n",
    "                                 end_date_year, end_date_month, end_date_day):\n",
    "    filen_ame = f'dev_history_{dev_id}_{start_date_year}-{start_date_month}-{start_date_day}__{end_date_year}-{end_date_month}-{end_date_day}.pkl'\n",
    "    file_path = f'{KRE_FT_PATH}{filen_ame}'\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Starting fetch from db for {dev_id}')\n",
    "        df = fetch_dev_history_by_from_till(dev_id,\n",
    "                                       start_date_year, start_date_month, start_date_day,\n",
    "                                       end_date_year, end_date_month, end_date_day)\n",
    "        df.to_pickle(path=file_path)\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe - Frequency Anomalities\n",
    "Implement the different probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from pandas.plotting import register_matplotlib_converters\n",
    "# register_matplotlib_converters()\n",
    "# import seaborn; seaborn.set()\n",
    "from scipy import signal\n",
    "# from matplotlib import rc\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "output_notebook()\n",
    "def find_tx_frequency_anomality(df_dev_data, dev_name = ''):\n",
    "    chart = df_dev_data[['time', 'amount']] #.set_index('time')\n",
    "    spend_chart = df_dev_data.loc[df_dev_data['tx_type'].isin(['spend','p2p'])]\n",
    "    spend_chart = spend_chart[['time', 'amount']]\n",
    "    \n",
    "    time_quants = chart.groupby(pd.Grouper(key='time', freq='300s', axis=1))\n",
    "    all_txs = time_quants.count()\n",
    "    spend_time_quants = spend_chart.groupby(pd.Grouper(key='time', freq='300s', axis=1))\n",
    "    spend_txs = spend_time_quants.count()\n",
    "    \n",
    "    print(f'spnd index size; {spend_txs.index.size}')\n",
    "    print(all_txs.shape)\n",
    "    print(spend_txs.shape)\n",
    "    \n",
    "    fig = plt.figure('Num of Tx every 5 minutes', figsize=(30,30))\n",
    "\n",
    "    # butterworth filter\n",
    "    #Creation of the filter            \n",
    "    mins = 0\n",
    "    bestbut = None\n",
    "    bestsup = None\n",
    "    bestrat = None\n",
    "    # sliding avg every hour => window = 12 since each smple is a 5 minutes aggregation\n",
    "    spend_txs['MA'] = spend_txs['amount'].rolling(window=12).mean().fillna(10)\n",
    "\n",
    "    # To tune these values use the train loops bellow\n",
    "    N = 4   # Filter order\n",
    "    fc = 0.05\n",
    "    lim = 2.65\n",
    "    b, a = signal.butter(N, fc)\n",
    "    butf = signal.filtfilt(b,a, spend_txs['amount'])\n",
    "    ratios = spend_txs['amount']/butf\n",
    "\n",
    "    \"\"\" use this to train the filter\n",
    "    for fq in np.arange(0.05, 0.95, 0.05):\n",
    "        for N in range(2,7):\n",
    "            fc = fq  # Cutoff frequency, normalized \n",
    "            b, a = signal.butter(N, fc)\n",
    "            butf = signal.filtfilt(b,a, all_txs['amount'])\n",
    "            ratios = all_txs['amount']/butf\n",
    "            for lims in [(5, 20), (2.5, 100), (1.5, 200), (1.2, 100)]:\n",
    "                lim, topTx = lims\n",
    "                suspects = all_txs.index[(lim < ratios) & (all_txs['amount'] > topTx)]\n",
    "                num_of_sspcts = suspects.shape[0]\n",
    "                if (num_of_sspcts > 0) and (num_of_sspcts < 100):\n",
    "                    if mins < num_of_sspcts:\n",
    "                        mins = num_of_sspcts\n",
    "                        bestbut = butf\n",
    "                        bestsup = suspects\n",
    "                        bestrat = ratios\n",
    "                        print(f'N: {N}, Freq: {fq}, lim: {lim}')\n",
    "\n",
    "    butf = bestbut\n",
    "    ratios = bestrat # all_txs['amount']/butf\n",
    "    \"\"\"\n",
    "            \n",
    "    #     suspects = bestsup # all_txs.index[(bestlim < ratios)]\n",
    "    # tx ~ 200 . --> ratio 2\n",
    "    # Different limits    \n",
    "    suspects = spend_txs.index[(spend_txs['amount'] > 500) &  ## only when there where more than x tx\n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 1.2) | ## Ratio to avg gt x\n",
    "                                   (spend_txs['amount'] > 200) &  \n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 1.5) | \n",
    "                                   (spend_txs['amount'] > 100) &  \n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 2.5) | \n",
    "                                   (spend_txs['amount'] > 20) &  \n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 5)] \n",
    "    print(suspects.shape)\n",
    "\n",
    "    p = figure(x_axis_type=\"datetime\", title=f\"Transactions Count every 5 seconds {dev_name}\", plot_width=600, plot_height=600)\n",
    "    p.x('time', 'amount', source=all_txs, size=2, legend=\"Num of Transactions\")\n",
    "    p.circle('time', 'amount', source=spend_txs, fill_color='yellow', size=4, legend=\"Num of Spends\")    \n",
    "    p.line(spend_txs.index, butf, line_color='blue', line_dash=[2, 5], legend=\"BW Filter\")\n",
    "    p.line('time', 'MA', source=spend_txs, line_color='red', line_dash=[1, 2], legend=\"Moving Avg\")\n",
    "    p.xaxis[0].formatter.days = '%m/%d/%Y'\n",
    "    p.xaxis.major_label_orientation = np.pi/3\n",
    "    show(p)\n",
    "\n",
    "    return {'spends':spend_txs, 'suspects':suspects}\n",
    "\n",
    "def find_tx_large_scale_anomality(df_dev_data, dev_name = ''):\n",
    "    chart = df_dev_data[['time', 'amount']]\n",
    "    spend_chart = df_dev_data.loc[df_dev_data['tx_type'].isin(['spend','p2p'])]\n",
    "    spend_chart = spend_chart[['time', 'amount']]\n",
    "    \n",
    "    time_quants = chart.groupby(pd.Grouper(key='time', freq='4H', axis=1))\n",
    "    all_txs = time_quants.count()\n",
    "    spend_time_quants = spend_chart.groupby(pd.Grouper(key='time', freq='4H', axis=1))\n",
    "    spend_txs = spend_time_quants.count()\n",
    "    \n",
    "    print(f'spnd index size; {spend_txs.index.size}')\n",
    "    print(all_txs.shape)\n",
    "    print(spend_txs.shape)\n",
    "    \n",
    "    fig = plt.figure('Num of Tx every 5 minutes', figsize=(30,30))\n",
    "\n",
    "    # butterworth filter\n",
    "    #Creation of the filter            \n",
    "            \n",
    "    mins = 0\n",
    "    bestbut = None\n",
    "    bestsup = None\n",
    "    bestrat = None\n",
    "    # sliding avg every hour => window = 12 since each smple is a 5 minutes aggregation\n",
    "    spend_txs['MA'] = spend_txs['amount'].rolling(window=6).mean().fillna(10)\n",
    "\n",
    "    # To tune these values use the train loops bellow\n",
    "    N = 3   # Filter order\n",
    "    fc = 0.05\n",
    "    lim = 2.5\n",
    "    b, a = signal.butter(N, fc)\n",
    "    butf = signal.filtfilt(b,a, spend_txs['amount'])\n",
    "    ratios = spend_txs['amount']/butf\n",
    "\n",
    "    \"\"\" use this to train the filter\n",
    "    for fq in np.arange(0.05, 0.95, 0.05):\n",
    "        for N in range(2,9):\n",
    "            fc = fq  # Cutoff frequency, normalized \n",
    "            b, a = signal.butter(N, fc)\n",
    "            butf = signal.filtfilt(b,a, spend_txs['amount'])\n",
    "            ratios = spend_txs['amount']/butf\n",
    "            # In case of 50 tx, the ratio with the bw filter shall be above 5 to be interesting.\n",
    "            # 150 - 2.5 and so on...\n",
    "            for lims in [(5, 50), (2.5, 150), (1.5, 400), (1.2, 600)]:\n",
    "                lim, topTx = lims\n",
    "                suspects = spend_txs.index[(lim < ratios) & (spend_txs['amount'] > topTx)]\n",
    "                num_of_sspcts = suspects.shape[0]\n",
    "                if (num_of_sspcts > 0) and (num_of_sspcts < 100):\n",
    "                    if mins < num_of_sspcts:\n",
    "                        mins = num_of_sspcts\n",
    "                        bestbut = butf\n",
    "                        bestsup = suspects\n",
    "                        bestrat = ratios\n",
    "                        print(f'N: {N}, Freq: {fq}, lim: {lim}')\n",
    "\n",
    "    butf = bestbut\n",
    "    ratios = bestrat # all_txs['amount']/butf\n",
    "    \"\"\"\n",
    "            \n",
    "    #     suspects = bestsup # all_txs.index[(bestlim < ratios)]\n",
    "    # tx ~ 200 . --> ratio 2\n",
    "    # Different limits    \n",
    "    suspects = spend_txs.index[(spend_txs['amount'] > 500) &  ## only when there where more than x tx\n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 1.8) | ## Ratio to avg gt x\n",
    "                                   (spend_txs['amount'] > 200) &  \n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 2) | \n",
    "                                   (spend_txs['amount'] > 100) &  \n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 3) | \n",
    "                                   (spend_txs['amount'] > 20) &  \n",
    "                                   (spend_txs['amount']/spend_txs['MA'] > 5)] \n",
    "    print(suspects.shape)\n",
    "\n",
    "\n",
    "    p = figure(x_axis_type=\"datetime\", title=f\"Transactions Count every 4 hours {dev_name}\", plot_width=600, plot_height=600)\n",
    "    p.x('time', 'amount', source=all_txs, size=2, legend=\"Num of Transactions\")\n",
    "#     p.circle(spend_txs.index, spend_txs['amount'], fill_color='gray', size=3, legend=\"Num of Spends\")\n",
    "    p.circle('time', 'amount', source=spend_txs, fill_color='yellow', size=4, legend=\"Num of Spends\")\n",
    "    \n",
    "    p.line(spend_txs.index, butf, line_color='blue', line_dash=[2, 5], legend=\"BW Filter\")\n",
    "    p.line('time', 'MA', source=spend_txs, line_color='red', line_dash=[2, 7], legend=\"Moving Avg\")\n",
    "    p.xaxis[0].formatter.days = '%m/%d/%Y'\n",
    "    p.xaxis.major_label_orientation = np.pi/3\n",
    "    show(p)\n",
    "\n",
    "    # Find anomalities\n",
    "#     display(spend_txs.loc[suspects])\n",
    "#     display(spend_txs.loc[time>'2019-02-30'][suspects])\n",
    "    return {'spends':spend_txs, 'suspects':suspects}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe - Ping-Pongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DH.columns)\n",
    "\n",
    "\n",
    "def get_next_top_wallet(df_dev_data, column_name='source', ix=0):\n",
    "    grouper = df_dev_data.groupby([column_name])\n",
    "    sums = grouper.sum()\n",
    "    cnts = grouper.count()\n",
    "    \n",
    "    # detect most paying wallet\n",
    "    sorted_cnts = cnts.sort_values(by='tx_hash', axis=0, ascending=False).reset_index()\n",
    "#     display(sums.head(5))\n",
    "#     display(sorted_cnts.head(5))\n",
    "    \n",
    "    # Analyze wallet\n",
    "    busy_wallet = sorted_cnts.iloc[ix][column_name]\n",
    "    num_of_tx = sorted_cnts.iloc[ix]['tx_hash']\n",
    "    total_payed = sums.loc[busy_wallet]\n",
    "    avg_payment = total_payed/num_of_tx\n",
    "    all_payments_by_wallet = df_dev_data[df_dev_data[column_name]==busy_wallet]\n",
    "    median_payment = all_payments_by_wallet['amount'].median()\n",
    "    \n",
    "    print(f'Wallet {busy_wallet} made {num_of_tx} transactions paying {total_payed} KIN in total')\n",
    "    print(f'Avg payment: {avg_payment}. Median payment: {median_payment}')\n",
    "    \n",
    "\n",
    "get_next_top_wallet(DH, 'source')\n",
    "# detect_pingpongs(DH, 'destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def detect_pingpongs(df_dev_data):\n",
    "    # Sources\n",
    "    grouper = df_dev_data.groupby(['source'])\n",
    "#     src_sums = grouper.sum()\n",
    "    src_cnts = grouper.count()\n",
    "    sorted_src_cnts = src_cnts.sort_values(by='tx_hash', axis=0, ascending=False).reset_index()\n",
    "    src_cnts = None #not needed, release some memory\n",
    "    \n",
    "    # Destinations\n",
    "    grouper = df_dev_data.groupby(['destination'])\n",
    "#     dst_sums = grouper.sum()\n",
    "    dst_cnts = grouper.count()\n",
    "    sorted_dst_cnts = dst_cnts.sort_values(by='tx_hash', axis=0, ascending=False).reset_index()\n",
    "    dst_cnts = None #not needed, release some memory\n",
    "    \n",
    "    \n",
    "    display(sorted_dst_cnts.head(5))\n",
    "    display(sorted_src_cnts.head(5))\n",
    "    \n",
    "    list_of_payers = sorted_src_cnts['source'].to_list()\n",
    "    payments_made_to_payers = df_dev_data[df_dev_data['destination'].isin(list_of_payers)]\n",
    "    self_payments = df_dev_data[df_dev_data['destination'] == df_dev_data['source']]\n",
    "    print(df_dev_data.shape)\n",
    "    print(payments_made_to_payers.shape)\n",
    "    print(self_payments.shape)\n",
    "    display(self_payments)\n",
    "    \n",
    "    # cound pingpongs\n",
    "    pingpong_df = pd.DataFrame()\n",
    "    for six, srow in sorted_src_conts.itterrows():\n",
    "        pay_wallet = row['source']\n",
    "        for dix, drow in sorted_dst_conts.itterrows():\n",
    "            dst_wallet = row['destination']\n",
    "            \n",
    "    \n",
    "detect_pingpongs(DH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe - Wallets Cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "# from datetime import timezone as tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_epoch(df, time_column_name='time'):\n",
    "    # Transform dates to numbers \n",
    "    types = df.dtypes\n",
    "    time_type = types[time_column_name]\n",
    "    if time_type != np.float64:\n",
    "        print('TRANSFORMING DATES TO FLOATS')\n",
    "        df[time_column_name] = df[time_column_name].astype('int64')//1e9\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def create_tx_graph(df_dev_data, directed=False, attributed_graph=True):\n",
    "    df_renamed = df_dev_data\n",
    "    df_renamed.rename(columns={'source': 'src_w', 'destination': 'dst_w', 'time': 'tx_t'}, inplace = True)\n",
    "\n",
    "    # Transform dates to numbers \n",
    "    if attributed_graph:\n",
    "        convert_time_to_epoch(df_renamed, time_column_name='tx_t')\n",
    "        print(f\"Max time {df_renamed['tx_t'].max()}, min time {df_renamed['tx_t'].min()}\")\n",
    "        attrs = ['amount', 'tx_t']\n",
    "    else:\n",
    "        attrs = None\n",
    "    \n",
    "    if directed:\n",
    "        g_engine = nx.DiGraph\n",
    "    else:\n",
    "        g_engine = nx.Graph\n",
    "    grph = nx.from_pandas_edgelist(df_renamed,\n",
    "                                source='src_w',\n",
    "                                target='dst_w',\n",
    "                                create_using=g_engine,\n",
    "                                edge_attr=attrs)\n",
    "    return grph\n",
    "\n",
    "def analyze_cliques(grph):\n",
    "    clq_enumerator = nx.enumerate_all_cliques(grph)\n",
    "    num_of_cliques = 0\n",
    "    biggest_clique_size = 0\n",
    "    biggest_clique = None\n",
    "    for clique in clq_enumerator:\n",
    "        num_of_wallets = len(clique)\n",
    "        if num_of_wallets > 4:\n",
    "            num_of_cliques += 1\n",
    "        if num_of_wallets > biggest_clique_size:\n",
    "            biggest_clique_size = num_of_wallets\n",
    "            biggest_clique = clique\n",
    "    \n",
    "    print(f\"{'*'*4} number of cliques: {num_of_cliques} {'*'*4}\")\n",
    "    print(f\"The largest clique has {biggest_clique_size} wallets. \")\n",
    "    print('Wallets in clique:')\n",
    "    for w_node in biggest_clique:\n",
    "        print(f'n: {w_node}')\n",
    "    \n",
    "    print('-'*8)\n",
    "    return biggest_clique_size\n",
    "\n",
    "def loops_list(grph):\n",
    "    self_loops = nx.nodes_with_selfloops(grph)\n",
    "    loops_arr = []\n",
    "    if self_loops:\n",
    "        for sl in self_loops:\n",
    "            loops_arr.append(sl)\n",
    "\n",
    "    return loops_arr\n",
    "\n",
    "def has_loops(grph):\n",
    "    found_loops = False\n",
    "    self_loops = nx.nodes_with_selfloops(grph)\n",
    "    for sl in self_loops:\n",
    "        if sl:\n",
    "          found_loops = True\n",
    "          break\n",
    "    return found_loops\n",
    "    \n",
    "def find_core(grph, core_rank=None):\n",
    "    cores_graph = nx.k_core(grph, k=core_rank)\n",
    "          \n",
    "    return cores_graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data in monthly chunks\n",
    "Read data from BI in multi threads. Each thread downloads a different month and saves a pkl to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read monthly segmented data in parallel and save it to disk\n",
    "dev_id = 'l68b'\n",
    "paramsarr = get_monthly_segments(2019, 5, 1)\n",
    "print(paramsarr)\n",
    "args_tups_arr = ((dev_id, dsy, dsm, dsd, dey, dem, ded) for dsy, dsm, dsd, dey, dem, ded in paramsarr)\n",
    "with PoolExecutor(max_workers=4) as executor:\n",
    "    for result in executor.map(lambda p: get_dev_history_by_from_till(*p), args_tups_arr):   # (*p) does the unpacking part\n",
    "        pass\n",
    "\n",
    "print(f\"{'*'*40} ALL DONE {'*'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct DF using existing monthly segments cached files.\n",
    "# If the pkl files don't exist, first download them from BQ using the parallel connections above ...\n",
    "dfs = []\n",
    "dev_id = 'l83h'\n",
    "print(dev_name_by_id(dev_id))\n",
    "\n",
    "paramsarr = get_monthly_segments(2019, 4, 1)\n",
    "print(paramsarr)\n",
    "args_tups_arr = ((dev_id, dsy, dsm, dsd, dey, dem, ded) for dsy, dsm, dsd, dey, dem, ded in paramsarr)\n",
    "with PoolExecutor(max_workers=1) as executor:\n",
    "    for result in executor.map(lambda p: get_dev_history_by_from_till(*p), args_tups_arr):   # (*p) does the unpacking part\n",
    "        dfs.append(result)\n",
    "        print(end='.')\n",
    "        pass\n",
    "\n",
    "DH = pd.concat(dfs)\n",
    "print(f\"{'*'*40} ALL DONE {'*'*40}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data in daily chunks\n",
    "Read data from BI in parallel and save it to the disk in pkl files.<br>\n",
    "Then read it searially and rebuild a huge DF."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Read from database adjacent time segments in parallel and save each segment to disk\n",
    "# NOTE this is not all April, but for checking now it's enough.\n",
    "dev_id = 'l83h'\n",
    "print(f'Caching Dev Data - {dev_name_by_id(dev_id)}')\n",
    "# paramsarr = get_weekly_segments(2019, 4, 1,\n",
    "#                         2019, 5, 1)\n",
    "\n",
    "paramsarr = get_n_days_segments(2019, 5, 15,\n",
    "                        2019, 5, 31, number_of_days=1)\n",
    "print(paramsarr)\n",
    "\n",
    "args_tups_arr = ((dev_id, dsy, dsm, dsd, dey, dem, ded) for dsy, dsm, dsd, dey, dem, ded in paramsarr)\n",
    "with PoolExecutor(max_workers=6) as executor:\n",
    "    for result in executor.map(lambda p: get_dev_history_by_from_till(*p), args_tups_arr):   # (*p) does the unpacking part\n",
    "        pass\n",
    "\n",
    "print(f\"{'*'*40} ALL DONE {'*'*40}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsarr = get_n_days_segments(2019, 3, 1,\n",
    "                        2019, 4, 1, number_of_days=1)\n",
    "display(paramsarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a DF from saved segmented dataframe to disk from parallel logic above (daily segments)\n",
    "dev_id = 'l83h'\n",
    "print(f'Reconstruct Dev Data - {dev_name_by_id(dev_id)}')\n",
    "paramsarr = get_n_days_segments(2019, 5, 1,\n",
    "                        2019, 5, 21, number_of_days=4)\n",
    "args_tups_arr = ((dev_id, dsy, dsm, dsd, dey, dem, ded) for dsy, dsm, dsd, dey, dem, ded in paramsarr)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "with PoolExecutor(max_workers=1) as executor:\n",
    "    for result in executor.map(lambda p: get_dev_history_by_from_till(*p), args_tups_arr):   # (*p) does the unpacking part\n",
    "        dfs.append(result)\n",
    "        print(end='.')\n",
    "        pass\n",
    "\n",
    "DH = pd.concat(dfs)\n",
    "dfs = []\n",
    "print(f\"{'*'*40} ALL DONE {'*'*40}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save DG using huge files format\n",
    "store = pd.HDFStore(KRE_FT_PATH + f'store_{dev_id}_2019-04-01_2019-05-05.h5')\n",
    "store['dev_history'] = DH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### TESTING AREA ####\n",
    "dev_id = '8vlz'\n",
    "DL = get_devs_list()\n",
    "print(dev_name_by_id('sss'))\n",
    "print(dev_name_by_id(dev_id))\n",
    "display(DL)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(DH.shape)\n",
    "display(DH['time'].head(2))\n",
    "DH = DH.sort_values(by=['time'], axis=0)\n",
    "display(DH['time'].head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data in chunks using the KRE package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kre.data.data_formater import dev_history_in_time_segmets, read_dev_lookup\n",
    "from kre.data.data_interpreter import DataInterpreter\n",
    "import datetime as dt\n",
    "\n",
    "dev_lookup = read_dev_lookup()\n",
    "display(dev_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_id = '8vlz'\n",
    "dev_name = DataInterpreter.dev_name_with_id(dev_lookup, dev_id)\n",
    "start_date = dt.datetime(2019, 5, 1)\n",
    "end_date = dt.datetime(2019, 7, 7)\n",
    "\n",
    "print(f'Loading history of {dev_name} between {start_date} and {end_date}')\n",
    "DH = dev_history_in_time_segmets(\n",
    "            dev_id,\n",
    "            start_date,\n",
    "            end_date,\n",
    "            segment_length_in_days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe - fq anomalities detect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_name = dev_name_by_id(dev_id)\n",
    "dev_name = DataInterpreter.dev_name_with_id(dev_lookup, dev_id)\n",
    "\n",
    "print(dev_name)\n",
    "results = find_tx_frequency_anomality(DH, dev_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_spends = results['spends']\n",
    "suspects = results['suspects']\n",
    "monthly_spends = monthly_spends.loc[suspects[suspects>='2019-03-01 00:00:00+0000']]\n",
    "# display(monthly_spends[monthly_spends['amount']>20])\n",
    "display(monthly_spends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = find_tx_large_scale_anomality(DH, dev_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monthly_spends = results['spends']\n",
    "suspects = results['suspects']\n",
    "# display(suspects[1])\n",
    "# monthly_spends = monthly_spends.loc[suspects]\n",
    "monthly_spends = monthly_spends.loc[suspects[suspects>='2019-03-01 00:00:00+0000']]\n",
    "display(monthly_spends[monthly_spends['amount']>40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe - Cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dev_name = dev_name_by_id(dev_id)\n",
    "print(dev_name)\n",
    "display(DH.head(2))\n",
    "print(DH.dtypes)\n",
    "print(DH.shape)\n",
    "graph = create_tx_graph(DH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest = analyze_cliques(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if biggest > 5:\n",
    "    # Create a Directed Graph\n",
    "    di_graph = create_tx_graph(DH, True)\n",
    "    \n",
    "    # Generate a sub graph from the clique\n",
    "    # Takes ages  clique_graph = nx.make_max_clique_graph(graph, create_using=nx.DiGraph)\n",
    "    \n",
    "    # nx.write_gml(G, f'/Users/skulas/Dev/TEMP/kin_graph_history_{num_of_txs}_g.gml')\n",
    "    filename = f'graph_of_biggest_clique_of_{dev_id}_size_{biggest}.graphml'\n",
    "    nx.write_graphml_lxml(clique_graph, f'/Users/skulas/Dev/TEMP/{filename}')\n",
    "    \n",
    "    # Graph of DS for a the period of time being used\n",
    "#     filename = f'graph_of_{dev_id}_clique_size_{biggest}.graphml'\n",
    "#     nx.write_graphml_lxml(di_graph, f'/Users/skulas/Dev/TEMP/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_graph = create_tx_graph(DH, True)\n",
    "loops_found = has_loops(di_graph)\n",
    "#     print(f'There are {len(self_loops)} SELF LOOPS IN THE {dev_id} GRAPH!')\n",
    "if loops_found:\n",
    "    self_loops = loops_list(di_graph)\n",
    "    print(f'FOUND {len(self_loops)} LOOPS. THIS IS A PROBLEM')\n",
    "    for sl in self_loops:\n",
    "        print(sl)\n",
    "    print('REMOVING LOOPS')\n",
    "    di_graph.remove_edges_from(nx.selfloop_edges(di_graph))\n",
    "\n",
    "# core_gaph = find_core(di_graph)\n",
    "# print(f'Number of nodes in entire graph: {di_graph.order()}')\n",
    "# print(f'Number of nodes in core: {core_gaph.order()}')\n",
    "# filename = f'CORE_graph_of_{dev_id}_.graphml'\n",
    "# nx.write_graphml_lxml(core_gaph, f'/Users/skulas/Dev/TEMP/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe - Cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kre.validation.fraud.probes.cyclic_activity import CyclicTransactions as CT\n",
    "from kre.data.data_interpreter import DataInterpreter\n",
    "\n",
    "cyclic = CT(DH, preset = CT.Preset.ROLL_5_TX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# wallet = 'GBAXSKHOWEXQ2COAACUSCC2ZMFXYGN23ZXYSY4EZSBBX2MQU4VDN6WVO'\n",
    "# wallet = 'GCNIOEWQZZMMEUGDR6GDLBRMFP3J3D5MIR2M5ZZETE6AU6BQUHSWTOLK'\n",
    "wallet = 'GA3WTR6HZYL4BNX3UN3LGUPSVOEPPSXX5O47ZVWVHM76R6PI55XSODHQ'\n",
    "wallet_activity = cyclic.analyze_wallet(wallet, statistic_summary=False)\n",
    "wallet_activity['date'] = wallet_activity['time'].dt.date\n",
    "# display(pd.to_datetime(wallet_activity['time'], yearfirst=True))\n",
    "# goups_destination = wallet_activity.groupby('destination')\n",
    "date_groups = wallet_activity.groupby(['destination', 'date'])\n",
    "# number_of_different_wallets = goups_destination.count()\n",
    "number_of_different_wallets = date_groups.count()\n",
    "display(number_of_different_wallets)\n",
    "\n",
    "# wlt = 'GCU6X575CDOYPHQ2RFU4JOK6OAWK7FE5IMLZR5QGEUK7P3CGFOWQ6SHU'\n",
    "# wlt_data = wallet_activity.loc[wallet_activity['destination'] == wlt]\n",
    "# wlt_data['date'] = wlt_data['time'].dt.date\n",
    "# date_groups = wlt_data.groupby(['destination', 'date'])\n",
    "\n",
    "# display(date_groups.count())\n",
    "# display(DH.loc[DH['destination'] == wlt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probe - P2P Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from kre.validation.fraud.probes.p2p_fake_transactions import P2PFakeTransactions as fp2p\n",
    "import datetime as dt\n",
    "\n",
    "###########\n",
    "# Fake P2p\n",
    "fakep2p = fp2p(DH)\n",
    "(res, temp) = fakep2p.run()\n",
    "\n",
    "display(res)\n",
    "display(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p2p_sizes = temp.groupby(level='source').size()\n",
    "print(p2p_sizes['GDZZU7SZN67MKYUMB4RSQ2QK5HRYYGGE3L6JYVKIV7F47HA75C3VXO6E'])\n",
    "cnt = p2p_sizes.size\n",
    "print(f'There are {cnt} paying wallets')\n",
    "for size in range(0, 3):\n",
    "    print(f'{size} ... {p2p_sizes.index[size]} : {p2p_sizes[size]}')\n",
    "    \n",
    "mn = p2p_sizes.mean()\n",
    "md = p2p_sizes.median()\n",
    "print(f'Avg: {mn}, Med: {md}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('p365')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tapatalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('tapa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NearBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('8vlz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('kit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('l68b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('l83h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pause For"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('pgbv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('swel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Travellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('yqyf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('kik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KinTipBot(OUR STAR) - Chaincity ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('rced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pop.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('lsff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tg = generate_report('ysa7')\n",
    "display(tg.head(3))\n",
    "mnw = tg['number_of_wallets'].mean()\n",
    "mnt = tg['number_of_tx'].mean()\n",
    "print('-'*80)\n",
    "print(f'Avg wllts {mnw}')\n",
    "print(f'Avg tx {mnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blast Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('ujti')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ThisThat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('2bpx')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "daily_histogram_data = tx_history['date'].astype(\"datetime64\")\n",
    "sfig = plt.figure('Tx Histogram', figsize=(16,9))\n",
    "daily_histogram = daily_histogram_data.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoChallenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_report('ze6y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Logic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Move along the differnet thresholds we test and ask one after the other in different order (random forest) to improve fraud prediction based on forest training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "291px",
    "width": "369px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
